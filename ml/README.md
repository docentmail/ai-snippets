<p style="font-size: 26px;"> Machine Learning. </p> 

**The folder contains my projects, Kaggle competitions and Courses for Machine Learning.**

------------ Table of Contents -------------
- [Dota 2: Win Probability Prediction (Kaggle Competition)](#dota-2-win-probability-prediction-kaggle-competition)
  - [My Result of Kaggle Leaderboard](#my-result-of-kaggle-leaderboard)
  - [Used Technologies and Packages](#used-technologies-and-packages)
- ["Data Analysis and Machine Learning" Coursera.com. 8 weeks.](#data-analysis-and-machine-learning-courseracom-8-weeks)
  - [Syllabus overview](#syllabus-overview)


----- ----- ---- ----- ----- ----- ----- -----


## Dota 2: Win Probability Prediction (Kaggle Competition)
Go to the folder [dota_kaggle](dota_kaggle/README.md)

Predict Dota 2 match winner by the first 5 minutes of the game

[Kaggle Competition](https://www.kaggle.com/competitions/dota-2-win-probability-prediction/overview)
The training set consists of matches, for which all of the ingame events (like kills, item purchase etc.) as well as match outcome are know. You are given only the first 5 minutes of each match and you need to predict the likelihood of Radiant victory.



### My Result of Kaggle Leaderboard
**169** out of 810 (in the first 20%)
![alt text](dota_kaggle/_readme001.png)

### Used Technologies and Packages

IPython.display, datetime, math, numpy, pandas, scipy.sparse, scipy.stats, sklearn.cross_validation, sklearn.ensemble, sklearn.grid_search, sklearn.linear_model, sklearn.metrics, sklearn.preprocessing, sklearn.tree, sys, warnings.


## "Data Analysis and Machine Learning" Coursera.com. 8 weeks.

**[<span style="color: green;">View certificate</span>](https://www.coursera.org/account/accomplishments/certificate/VY63EBXE35FR)**.

Go to the folder [coursera_ml](coursera_ml/README.md)

### Syllabus overview

- **Logical Classification Methods**: Decision Tree  
- **Metric Classification Methods**: Nearest Neighbor, Parzen Window  
- **Linear Classification Methods**:  
  - Stochastic Gradient  
  - Gradient Numerical Minimization Methods  
  - SG6, SAG3 Algorithms  
  - Feature Normalization  
- **Dimension Reduction and Principal Component Analysis**  
- **Linear Regression**:  
  - Singular Value Decomposition  
  - Ridge Regression  
  - LASSO  
- **Compositions of Algorithms**:  
  - Bagging and Random Forest  
  - Gradient Boosting: Modifications and Heuristics  
- **Neural Networks**:  
  - Backpropagation  
  - Standard Heuristics  
- **Clustering and Visualization**:  
  - Hierarchical Clustering  
  - Nonlinear Dimensionality Reduction Methods  
- **Partial Training**: Applying Clustering and Classification to Partial Learning Problems

Python, numpy, pandas, GradientBoostingClassifier, LogisticRegression, sklearn.cross_validation, StandardScaler, json, bz2, AUC-ROC, datetime, scipy.sparse, sklearn.ensemble, sklearn.grid_search, sklearn.linear_model, sklearn.preprocessing, warnings, train_test_split, numpy, log_loss, GradientBoostingClassifier, matplotlib, RandomForestClassifier, RandomForestRegressor, KFold, PCA, DictVectorizer, TfidfVectorizer, hstack, Ridge, SVC, accuracy_score, roc_auc_score, precision_recall_curve, Scikit-learn, Matplotlib, Math, Sys, sklearn.model_selection, sklearn.neighbors, collections, klearn.tree.DecisionTreeClassifier, matplotlib.pyplot, sklearn.metrics.accuracy_score, SciPy.stats.pearsonr, re, collections
