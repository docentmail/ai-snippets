{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization reservs\n",
    "- average for градиенный бустинг и линейную регрессию\n",
    "- убрать время начала и номер матча\n",
    "- сделать чтобы все матчи начинались с 0. Без отрицательных времен\n",
    "- продумать с пустыми значениями - их очень много\n",
    "- (Done 01 - use it) first_blood_time', 'first_blood_team' - алгоритм понимает что это связка???? только время - (+) для одной команды и (-) для другой   - Only worse!!!\n",
    "- посчитать \"производительность\" команд каждую минуту. Может динамика поможет. \n",
    "- покуртить ручки алгоритмов - может поможет\n",
    "- + добавить степени числовых признаков - выйти за линейность - **2 для градиентного бустинга\n",
    "- сколько -1 комнат в списке\n",
    "- (Done modify_dayaframe_03 - use it) попробовать просуммировать однотипные поля команды\n",
    "\n",
    "Development convinience\n",
    " - small subset to test that code does not throw exceptions  - done\n",
    " - reasonable file naming \n",
    " - self documented results\n",
    " - one button run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on 1000 - best param =  {'C': 0.00080000000000000004} best score =  0.747402682882\n",
    "\n",
    "# configuration variable\n",
    "number_rows_to_use= -1 # 100   # -1 all records\n",
    "\n",
    "# annoying warnings disabling\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# global variables\n",
    "heroes = [\n",
    "    'r1_hero',\n",
    "    'r2_hero',\n",
    "    'r3_hero',\n",
    "    'r4_hero',\n",
    "    'r5_hero',\n",
    "    'd1_hero',\n",
    "    'd2_hero',\n",
    "    'd3_hero',\n",
    "    'd4_hero',\n",
    "    'd5_hero'\n",
    "]\n",
    "fields_to_remove = [\n",
    "    'r1_hero',\n",
    "    'r2_hero',\n",
    "    'r3_hero',\n",
    "    'r4_hero',\n",
    "    'r5_hero',\n",
    "    'd1_hero',\n",
    "    'd2_hero',\n",
    "    'd3_hero',\n",
    "    'd4_hero',\n",
    "    'd5_hero',\n",
    "    'lobby_type'\n",
    "#    'start_time'\n",
    "]\n",
    "\n",
    "import datetime\n",
    "\n",
    "def printtime():\n",
    "    return datetime.datetime.utcnow()\n",
    "\n",
    "\n",
    "# Удалите признаки, связанные с итогами матча (они помечены в описании данных как отсутствующие в тестовой выборке).\n",
    "'''\n",
    "Итог матча (данные поля отсутствуют в тестовой выборке, поскольку содержат информацию, выходящую за пределы первых 5 минут матча)\n",
    "duration: длительность\n",
    "radiant_win: 1, если победила команда Radiant, 0 — иначе\n",
    "Состояние башен и барраков к концу матча (см. описание полей набора данных)\n",
    "tower_status_radiant\n",
    "tower_status_dire\n",
    "barracks_status_radiant\n",
    "barracks_status_dire\n",
    "'''\n",
    "\n",
    "# usage: match_info, y = remove_result_columns(features)\n",
    "def remove_result_columns(the_features):\n",
    "    print \"remove_result_columns (Before): shape\",the_features.shape\n",
    "    rez_col_names = [\n",
    "    'duration',\n",
    "    'radiant_win',\n",
    "    'tower_status_radiant',\n",
    "    'tower_status_dire',\n",
    "    'barracks_status_radiant',\n",
    "    'barracks_status_dire'\n",
    "    ]\n",
    "    # res_info = features[rez_col_names]\n",
    "    removed_rez = the_features.drop(rez_col_names, axis=1)\n",
    "    print \"remove_result_columns (After): shape\",removed_rez.shape\n",
    "    return removed_rez, the_features['radiant_win']\n",
    "\n",
    "\n",
    "# remove initial heroes and lobbies columns\n",
    "def remove_initial_heroes_and_lobbies(parX):\n",
    "    print parX.shape, type(parX)\n",
    "    X_categor_removed= parX.drop( heroes+ ['lobby_type'], axis=1)\n",
    "    print X_categor_removed.shape, type(X_categor_removed)\n",
    "    return X_categor_removed\n",
    "\n",
    "'''\n",
    "Замените пропуски на нули с помощью функции fillna(). \n",
    "На самом деле этот способ является предпочтительным для логистической регрессии, поскольку он позволит пропущенному \n",
    "значению не вносить никакого вклада в предсказание. Для деревьев часто лучшим вариантом оказывается замена пропуска \n",
    "на очень большое или очень маленькое значение — в этом случае при построении разбиения вершины можно будет отправить \n",
    "объекты с пропусками в отдельную ветвь дерева. Также есть и другие подходы — например, замена пропуска на среднее \n",
    "значение признака. Мы не требуем этого в задании, но при желании попробуйте разные подходы к обработке пропусков и \n",
    "сравните их между собой.\n",
    "'''\n",
    "# Замените пропуски на нули с помощью функции fillna().\n",
    "def replace_skpped_values_with_0(theX):\n",
    "    print 'Before'\n",
    "    print 'NaNs =' ,np.count_nonzero(np.isnan(theX))\n",
    "    print 'not NaNs =' ,np.count_nonzero(~np.isnan(theX))\n",
    "    rez= theX.fillna(0)\n",
    "    print 'after'\n",
    "    print 'NaNs =' ,np.count_nonzero(np.isnan(rez))\n",
    "    print 'not NaNs =' ,np.count_nonzero(~np.isnan(rez))\n",
    "    return rez\n",
    "\n",
    "'''\n",
    "Среди признаков в выборке есть категориальные, которые мы использовали как числовые, что вряд ли является хорошей \n",
    "идеей. Категориальных признаков в этой задаче одиннадцать: lobby_type и r1_hero, r2_hero, ..., r5_hero, d1_hero, \n",
    "d2_hero, ..., d5_hero. Уберите их из выборки, и проведите кросс-валидацию для логистической регрессии на новой выборке \n",
    "с подбором лучшего параметра регуляризации. Изменилось ли качество? Чем вы можете это объяснить?\n",
    "'''\n",
    "\n",
    "# replace heroes with the bag of the words\n",
    "def words_bag_of_heroes(theX):\n",
    "    # calcule heroes range\n",
    "    expected_num=113\n",
    "    unique_heroes = np.unique(theX[heroes])\n",
    "    if (max(unique_heroes)>expected_num):\n",
    "        return 0 # throw exception here\n",
    "    if (min(unique_heroes)<1):\n",
    "        return 0 # throw exception here\n",
    "    X_pick = np.zeros((theX.shape[0], expected_num))\n",
    "    for i, match_id in enumerate(theX.index):\n",
    "        for p in xrange(5):\n",
    "            X_pick[i, theX.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "            X_pick[i, theX.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1\n",
    "    return X_pick\n",
    "\n",
    "\n",
    "# добавим мешок слов по комнате\n",
    "\n",
    "'''id,name\n",
    "-1,Invalid\n",
    "0,Public matchmaking\n",
    "1,Practice\n",
    "2,Tournament\n",
    "3,Tutorial\n",
    "4,Co-op with bots\n",
    "5,Team match\n",
    "6,Solo Queue\n",
    "7,Ranked\n",
    "8,Solo Mid 1vs1\n",
    "'''\n",
    "        \n",
    "#################################################################################################\n",
    "def words_bag_of_rooms(theX):\n",
    "    num_rooms = 10\n",
    "    # check rooms range\n",
    "    num_rooms_max = 8\n",
    "    num_rooms_min = -1\n",
    "    unique_lobbyes = np.unique(theX['lobby_type'])\n",
    "    if (max(unique_lobbyes)>num_rooms_max):\n",
    "        raise NameError(max(unique_lobbyes)+'too big')  \n",
    "    if (min(unique_lobbyes) < num_rooms_min):\n",
    "        raise NameError(max(unique_lobbyes)+'too small')\n",
    "    # делаем мешок слов\n",
    "    X_pick_room = np.zeros([theX.shape[0], num_rooms])   # !!! two (())\n",
    "    for i, match_id in enumerate(theX.index):\n",
    "        for p in xrange(10):\n",
    "            X_pick_room[i, theX.ix[match_id, 'lobby_type']+1] = 1    \n",
    "    return  X_pick_room       \n",
    "\n",
    "\n",
    "\n",
    "# join X_pick heroes and rooms\n",
    "def join_bags_rooms_and_heroes(parX_pick_heroes , parX_pick_rooms):\n",
    "    print parX_pick_heroes.shape, parX_pick_rooms.shape\n",
    "    rez=np.concatenate((parX_pick_heroes , parX_pick_rooms),  axis=1)\n",
    "    print rez.shape\n",
    "    return rez\n",
    "    \n",
    "    \n",
    "# add word bag to the DataSet\n",
    "from scipy.sparse import hstack\n",
    "def add_word_bag(parDataSet, parX_pick):\n",
    "    print parDataSet.shape, type(parDataSet)\n",
    "    print parX_pick.shape, type(parX_pick)\n",
    "    X_joined = hstack([parDataSet,parX_pick]).toarray()\n",
    "    print X_joined.shape, type(X_joined)\n",
    "    return X_joined \n",
    "\n",
    "'''\n",
    "<b>Важно</b>: не забывайте, что линейные алгоритмы чувствительны к масштабу признаков! Может пригодиться sklearn.preprocessing.StandartScaler.\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# usage\n",
    "# ss = stdScaling_fit(X)\n",
    "# X_std_scal = stdScaling_transform(X ,ss)\n",
    "def stdScaling_fit(theX):\n",
    "     return StandardScaler().fit(theX)\n",
    "\n",
    "def stdScaling_transform(theX, stdScaler):\n",
    "     return stdScaler.transform(theX)\n",
    "    \n",
    "    \n",
    "# calculate C for logistic regression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "def calculate_logreg_best_param(X_par, Y_par):\n",
    "    start_time = datetime.datetime.now()\n",
    "    param_grid = {'C': [0.00000000001, 0.0001, 0.001 ,0.01, 0.1, 1, 10, 100, 1000, 100000000] }\n",
    "\n",
    "    print \"--- initial C calculation\"\n",
    "    print param_grid\n",
    "    k_fold = KFold(len(Y_par), n_folds=5, shuffle=True, random_state=555)\n",
    "    clf = LogisticRegression(penalty='l2', random_state=555, n_jobs=-1)\n",
    "    gs_cv = GridSearchCV(clf, param_grid, scoring='roc_auc', cv=k_fold, n_jobs=-1)\n",
    "    gs_cv.fit(X_par, Y_par)\n",
    "    print \"best param = \", gs_cv.best_params_, \"best score = \", gs_cv.best_score_\n",
    "    for score in gs_cv.grid_scores_:\n",
    "        print score\n",
    "    # fine tuning for \n",
    "    fine_grid = {'C': np.add(gs_cv.best_params_.get('C'), np.multiply(np.arange(-5, 6), gs_cv.best_params_.get('C')/10))}\n",
    "    print \"--- fine C tuning\"\n",
    "    print fine_grid\n",
    "    gs_cv = GridSearchCV(clf, fine_grid, scoring='roc_auc', cv=k_fold)\n",
    "    gs_cv.fit(X_par, Y_par)\n",
    "    print \"best param = \", gs_cv.best_params_, \"best score = \", gs_cv.best_score_\n",
    "    for score in gs_cv.grid_scores_:\n",
    "        print score\n",
    "    print 'Average time for single regression fit:', (datetime.datetime.now() - start_time)/(len(param_grid.get('C')) +len(fine_grid.get('C')))\n",
    "    return gs_cv.best_params_\n",
    "\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "import datetime\n",
    "\n",
    "# cross validation of GradientBoostingClassifier \n",
    "def calculate_scores_for_estimators(n_estimators, X, y, max_depth=3):\n",
    "    print \"calculate_scores_for_estimators: start \", printtime()\n",
    "    start_time = datetime.datetime.now()\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "        random_state=555, max_depth=max_depth\n",
    "    )\n",
    "    k_fold = KFold(len(y), n_folds=5, random_state=555, shuffle=True)\n",
    "    local_scores = cross_val_score(clf, X, y, cv=k_fold, scoring='roc_auc', n_jobs=-1)\n",
    "    print 'score_mean=',local_scores.mean(), 'scores_std=',local_scores.std(), \\\n",
    "        'n_estimators =', n_estimators, 'max_depth=', max_depth, 'Time:', datetime.datetime.now() - start_time\n",
    "    print \"calculate_scores_for_estimators: finish \", printtime()\n",
    "    return local_scores.mean(), local_scores.std(), n_estimators, max_depth\n",
    "\n",
    "def crossvalidate_estimators(arr_n_estimators, X, y, max_depth=3):\n",
    "    print \"crossvalidate_estimators: start \", printtime()\n",
    "    rez_scores_local = list()\n",
    "    for n_estimators in arr_n_estimators:    \n",
    "        rez_scores_local.append(calculate_scores_for_estimators(n_estimators, X , y , max_depth=max_depth))    \n",
    "    print \"crossvalidate_estimators: finish \", printtime()    \n",
    "    return rez_scores_local\n",
    "\n",
    "def get_best_GradientBoostingClassifier(rez_scores_local):\n",
    "    # get best classifier\n",
    "    import sys\n",
    "    print type(rez_scores_local)\n",
    "    type(rez_scores_local[0])\n",
    "    max_score=-sys.maxint - 1\n",
    "    max_index=\"a\"\n",
    "    for index in range(len(rez_scores_local)):\n",
    "        if (rez_scores_local[index][0]>max_score):\n",
    "            max_index=index\n",
    "            max_score=rez_scores_local[index][0]\n",
    "            \n",
    "        print rez_scores_local[index][0]\n",
    "    best_n_estimators=rez_scores_local[max_index][2]\n",
    "    best_max_depth=rez_scores_local[max_index][3]\n",
    "    \n",
    "    print 'max_score =', max_score, 'max_index=', max_index, \\\n",
    "        'best_n_estimators=', best_n_estimators, 'best_max_depth=', best_max_depth\n",
    "    \n",
    "    clf = GradientBoostingClassifier(n_estimators=best_n_estimators,\n",
    "        random_state=555, max_depth=best_max_depth\n",
    "    )\n",
    "    return clf\n",
    "\n",
    "# calculate predicted probabilities and store to file\n",
    "#fileName = '/Volumes/fast64/My/MachineLearning/week07/Dota2_01/dota2-kaggle-01.csv'\n",
    "def store_predicted_probability(clf, parX, parX_match_id, fileName):\n",
    "    rez=clf.predict_proba(parX)[:, 1]\n",
    "    print \"Минимум предсказанных вероятностей =\",min(rez)\n",
    "    print \"Максимум предсказанных вероятностей =\", max(rez)\n",
    "    check = {\n",
    "        'match_id': parX_match_id.index, \n",
    "        'radiant_win': rez\n",
    "    }\n",
    "\n",
    "    check_dframe = pandas.DataFrame.from_dict(check)\n",
    "    check_dframe.set_index('match_id')\n",
    "\n",
    "    check_dframe.to_csv(fileName, index=False, columns=['match_id', 'radiant_win'])\n",
    "    return check_dframe    \n",
    "\n",
    "\n",
    "# prepare data for logistic regression\n",
    "import pandas\n",
    "def prepare_for_log_regr(fileName, isTest):\n",
    "    # Считайте таблицу с признаками из файла features.csv с помощью кода, приведенного выше. \n",
    "\n",
    "    features = pandas.read_csv(fileName, index_col='match_id')\n",
    "    if (number_rows_to_use>0):\n",
    "        print \"future trancated !!!!!!!!!!  to rows \", number_rows_to_use\n",
    "        # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "        features = features.sample(n=number_rows_to_use, random_state=555)\n",
    "\n",
    "    features = modify_dayaframe_01(features)\n",
    "    features = modify_dayaframe_03(features)\n",
    "    features = modify_dayaframe_02(features)\n",
    "    \n",
    "    \n",
    "    # print features.head()\n",
    "    print features.info()\n",
    "    \n",
    "    if (isTest):        \n",
    "        local_match_info = features\n",
    "    else:    \n",
    "        local_match_info, local_y = remove_result_columns(features)\n",
    "   \n",
    "    # calculate_importance(local_match_info, local_y)\n",
    "        \n",
    "    X_removed_heroes_rooms = remove_initial_heroes_and_lobbies(local_match_info)\n",
    "    X = replace_skpped_values_with_0(X_removed_heroes_rooms)\n",
    "\n",
    "    xxx_heroes= words_bag_of_heroes(local_match_info)\n",
    "    print type(xxx_heroes)\n",
    "    print xxx_heroes.shape\n",
    "\n",
    "    xxx_rooms= words_bag_of_rooms(local_match_info)\n",
    "    print type(xxx_rooms)\n",
    "    print xxx_rooms.shape\n",
    "\n",
    "    xxx_rooms_and_heroes=join_bags_rooms_and_heroes(xxx_heroes , xxx_rooms)\n",
    "\n",
    "    X_bagged = add_word_bag(X,xxx_rooms_and_heroes)\n",
    "    if (isTest):        \n",
    "        return X_bagged, local_match_info\n",
    "    else:    \n",
    "        return X_bagged, local_y, local_match_info\n",
    "\n",
    "    \n",
    "# prepare data for gradient boosting\n",
    "import pandas\n",
    "def prepare_for_gradient_boosting(fileName, isTest):\n",
    "    # Считайте таблицу с признаками из файла features.csv с помощью кода, приведенного выше. \n",
    "\n",
    "    features = pandas.read_csv(fileName, index_col='match_id')\n",
    "    if (number_rows_to_use>0):\n",
    "        print \"future trancated !!!!!!!!!!  to rows \", number_rows_to_use\n",
    "        # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "        features = features.sample(n=number_rows_to_use, random_state=555)\n",
    "\n",
    "    features = modify_dayaframe_01(features)\n",
    "    features = modify_dayaframe_03(features)\n",
    "    features = modify_dayaframe_02(features)\n",
    "    \n",
    "    \n",
    "    # print features.head()\n",
    "    print features.info()\n",
    "    \n",
    "    if (isTest):        \n",
    "        local_match_info = features\n",
    "    else:    \n",
    "        local_match_info, local_y = remove_result_columns(features)\n",
    "   \n",
    "    # calculate_importance(local_match_info, local_y)\n",
    "    X = replace_skpped_values_with_0(local_match_info)\n",
    "    '''\n",
    "    X_removed_heroes_rooms = remove_initial_heroes_and_lobbies(local_match_info)\n",
    "    X = replace_skpped_values_with_0(X_removed_heroes_rooms)\n",
    "\n",
    "    xxx_heroes= words_bag_of_heroes(local_match_info)\n",
    "    print type(xxx_heroes)\n",
    "    print xxx_heroes.shape\n",
    "\n",
    "    xxx_rooms= words_bag_of_rooms(local_match_info)\n",
    "    print type(xxx_rooms)\n",
    "    print xxx_rooms.shape\n",
    "\n",
    "    xxx_rooms_and_heroes=join_bags_rooms_and_heroes(xxx_heroes , xxx_rooms)\n",
    "\n",
    "    X_bagged = add_word_bag(X,xxx_rooms_and_heroes)\n",
    "    '''    \n",
    "    if (isTest):        \n",
    "        return X, local_match_info\n",
    "    else:    \n",
    "        return X, local_y, local_match_info\n",
    "    \n",
    "# sample code to append column to pandas.core.frame.DataFrame\n",
    "def append_column_to_DataFrame():\n",
    "    print x_bust_dataframe.shape\n",
    "    x_bust_dataframe['new_field'] = x_bust_dataframe['r1_lh']**2\n",
    "\n",
    "    print x_bust_dataframe['new_field'], x_bust_dataframe['r1_lh']\n",
    "    print x_bust_dataframe.shape\n",
    "\n",
    "    x_bust_dataframe['new_field_1'] = x_bust_dataframe.apply(lambda row: min([row['r1_lh'], row['r2_lh']])-row['r3_lh'], axis=1)\n",
    "    print x_bust_dataframe.shape\n",
    "\n",
    "# sample code to append column to numpy.ndarray\n",
    "def append_column_to_ndarray():\n",
    "    print x_bust.shape\n",
    "    print x_bust[:,:1].shape\n",
    "    # print x_bust[:,:1]\n",
    "    new_x=np.append(x_bust, x_bust[:,:1]**2, 1)\n",
    "    new_x.shape\n",
    "\n",
    "'''\n",
    "# Best usage is - add new field and remove 2 sources\n",
    "# считает первую кровь со знаком в одном поле\n",
    "# first_blood_time: игровое время первой крови\n",
    "# first_blood_team: команда, совершившая первую кровь (0 — Radiant, 1 — Dire)\n",
    "first_blood_player1: игрок, причастный к событию\n",
    "first_blood_player2: второй игрок, причастный к событию\n",
    "=== for log_regression: 5000\n",
    "0.735882809137   – удалил все\n",
    "0.736234081207 - удалил только 2 расчетных поля\n",
    "# === for log_regression: 1000\n",
    "# 0.716864897388- add 'first_blood_time_team', remove 'first_blood_team','first_blood_time', 'first_blood_player1','first_blood_player2'  \n",
    "# 0.717485181078- add 'first_blood_time_team', remove 'first_blood_team','first_blood_time'  \n",
    "# 0.716784645812 add 'first_blood_time_team'.\n",
    "# 0.716864897388 - original   \n",
    "# === for Gradient Boosting: \n",
    "# with this transformation \n",
    "# max_score = 0.697781663605 max_index= 0 best_n_estimators= 200 best_max_depth= 6\n",
    "# original \n",
    "# max_score = 0.698382534947 max_index= 0 best_n_estimators= 200 best_max_depth= 6\n",
    "'''\n",
    "def modify_dayaframe_01(parDataFrame):\n",
    "    print 'modify_dayaframe_01 (Before). parDataFrame', parDataFrame.shape\n",
    "    parDataFrame['first_blood_time_team'] = \\\n",
    "        parDataFrame.apply(lambda row: row['first_blood_time']*(row['first_blood_team']*2-1) , axis=1)\n",
    "    # print 'modify_dayaframe_01 (Middle). parDataFrame', parDataFrame.shape    \n",
    "    # rezDataFrame=parDataFrame\n",
    "    #rezDataFrame=parDataFrame\n",
    "    rezDataFrame=parDataFrame.drop( ['first_blood_team','first_blood_time'], axis=1)\n",
    "    # rezDataFrame=rezDataFrame.drop( ['first_blood_player1','first_blood_player2'], axis=1)\n",
    "    print 'modify_dayaframe_01 (After). parDataFrame', rezDataFrame.shape\n",
    "    return rezDataFrame\n",
    "\n",
    "# add pow(x,) fields\n",
    "# === for log_regression (1000 records)\n",
    "# 0.729154975126 - original\n",
    "# 0.736179073034 ** 2 \n",
    "# 0.743773109605 ** 3\n",
    "# 0.743972802269 ** 4\n",
    "# === for log_regression:\n",
    "# 0.714259140933add math.sqrt and remove original gold - make it even worse\n",
    "# 0.721677111535 add **2 and remove original gold - make it even worse\n",
    "\n",
    "# 0.729154975126 with function - added **2 - worse than original\n",
    "# 0.716864897388 - original   (Better !!!!!!!!!!!)\n",
    "# === for Gradient Boosting: \n",
    "# with this **2 and removed original\n",
    "# score_mean= 0.699060828574 scores_std= 0.0242692498252 n_estimators = 200 max_depth= 6\n",
    "# with  **2 - better a little\n",
    "#max_score = 0.699533440786 max_index= 0 best_n_estimators= 200 best_max_depth= 6 !!!!!!!!!!!!\n",
    "# original \n",
    "# max_score = 0.698382534947 max_index= 0 best_n_estimators= 200 best_max_depth= 6\n",
    "\n",
    "import math\n",
    "def modify_dayaframe_02(parDataFrame):\n",
    "    print 'modify_dayaframe_02 (Before). parDataFrame', parDataFrame.shape\n",
    "    power_value=2\n",
    "    toPower=['r1_gold','r2_gold','r3_gold','r4_gold','r5_gold','r_gold', \\\n",
    "             'd1_gold','d2_gold','d3_gold','d4_gold','d5_gold','d_gold',\\\n",
    "            \n",
    "             'r1_level','r2_level','r3_level','r4_level','r5_level','r_level', \\\n",
    "             'd1_level','d2_level','d3_level','d4_level','d5_level','d_level',\\\n",
    "            \n",
    "             'r1_xp','r2_xp','r3_xp','r4_xp','r5_xp', 'r_xp', \\\n",
    "             'd1_xp','d2_xp','d3_xp','d4_xp','d5_xp', 'd_xp', \\\n",
    "            \n",
    "             'r1_lh','r2_lh','r3_lh','r4_lh','r5_lh', 'r_lh', \\\n",
    "             'd1_lh','d2_lh','d3_lh','d4_lh','d5_lh','d_lh', \\\n",
    "            \n",
    "             'r1_kills','r2_kills','r3_kills','r4_kills','r5_kills', 'r_kills', \\\n",
    "             'd1_kills','d2_kills','d3_kills','d4_kills','d5_kills', 'd_kills',\\\n",
    "            \n",
    "             'r1_items','r2_items','r3_items','r4_items','r5_items','r_items', \\\n",
    "             'd1_items','d2_items','d3_items','d4_items','d5_items','d_items' \\\n",
    "            ]\n",
    "\n",
    "    for index in range(len(toPower)):\n",
    "        pow_field(toPower[index], power_value,parDataFrame)\n",
    "        #parDataFrame[powered[index]] = \\\n",
    "        #    parDataFrame.apply(lambda row: math.sqrt(row[toPower[index]]), axis=1)\n",
    "        #parDataFrame[powered[index]] = \\\n",
    "        #    parDataFrame.apply(lambda row: row[toPower[index]]**2, axis=1)\n",
    "    rezDataFrame=parDataFrame\n",
    "    # rezDataFrame=parDataFrame.drop( toPower, axis=1)\n",
    "    print 'modify_dayaframe_02 (After). rezDataFrame', rezDataFrame.shape\n",
    "    return rezDataFrame\n",
    "\n",
    "def pow_field(fld_name, mx_pwr,parDataFrame):\n",
    "    for p in xrange(mx_pwr-1):\n",
    "        parDataFrame[fld_name+'pwr'+str(p+2)] = \\\n",
    "            parDataFrame.apply(lambda row: math.pow(row[fld_name], p+2), axis=1)\n",
    "\n",
    "# Best usage is - add new field and not remove sources\n",
    "# add summ of gold R and D fields\n",
    "# === for log_regression: Gold only\n",
    "# 0.703771390136  remove original gold - \n",
    "# 0.723777307057 add summ for gold R and D and remove original gold \n",
    "# 0.729734140515 add summ for gold only R and D  \n",
    "# === for log_regression: gold..items only\n",
    "# 0.742759042342 add summ for gold..items R and D  \n",
    "# 0.716864897388 - original   (Better !!!!!!!!!!!)\n",
    "# === for Gradient Boosting: \n",
    "import math\n",
    "def modify_dayaframe_03(parDataFrame):\n",
    "    print 'modify_dayaframe_03 (Before). parDataFrame', parDataFrame.shape\n",
    "    add_sum_field('r', '_gold', parDataFrame)\n",
    "    add_sum_field('d', '_gold', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_level', parDataFrame)\n",
    "    add_sum_field('d', '_level', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_xp', parDataFrame)\n",
    "    add_sum_field('d', '_xp', parDataFrame)\n",
    "\n",
    "\n",
    "    add_sum_field('r', '_lh', parDataFrame)\n",
    "    add_sum_field('d', '_lh', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_kills', parDataFrame)\n",
    "    add_sum_field('d', '_kills', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_deaths', parDataFrame)\n",
    "    add_sum_field('d', '_deaths', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_items', parDataFrame)\n",
    "    add_sum_field('d', '_items', parDataFrame)\n",
    "\n",
    "\n",
    "    '''\n",
    "    toPower=['r1_gold','r2_gold','r3_gold','r4_gold','r5_gold', \\\n",
    "             'd1_gold','d2_gold','d3_gold','d4_gold','d5_gold']\n",
    "    powered=['r1_gold2','r2_gold2','r3_gold2','r4_gold2','r5_gold2', \\\n",
    "             'd1_gold2','d2_gold2','d3_gold2','d4_gold2','d5_gold2']\n",
    "    parDataFrame['r_gold'] = parDataFrame['r1_gold']+ parDataFrame['r2_gold']+ parDataFrame['r3_gold'] + \\\n",
    "        parDataFrame['r4_gold']+parDataFrame['r5_gold']\n",
    "    parDataFrame['d_gold'] = parDataFrame['d1_gold']+ parDataFrame['d2_gold']+ parDataFrame['d3_gold'] + \\\n",
    "        parDataFrame['d4_gold']+parDataFrame['d5_gold']\n",
    "    '''\n",
    "    rezDataFrame=parDataFrame\n",
    "    # rezDataFrame=parDataFrame.drop( toPower, axis=1)\n",
    "    print 'modify_dayaframe_03 (After). rezDataFrame', rezDataFrame.shape\n",
    "    return rezDataFrame\n",
    "\n",
    "# add_sum_field('r', '_gold', parDataFrame)\n",
    "def add_sum_field(r_d, fd_name, parDataFrame):\n",
    "    parDataFrame[r_d+fd_name] = parDataFrame[r_d+'1'+fd_name]+ parDataFrame[r_d+'2'+fd_name]+ parDataFrame[r_d+'3'+fd_name] + \\\n",
    "        parDataFrame[r_d+'4'+fd_name]+parDataFrame[r_d+'5'+fd_name]\n",
    "\n",
    "'''\n",
    "        'r%d_hero' % (p+1)]\n",
    "            X_pick[i, theX.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "            X_pick[i, theX.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1\n",
    "'''    \n",
    "# \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def calculate_importance(theX, theY):\n",
    "    clf = DecisionTreeClassifier(random_state=241)\n",
    "    clf.fit(theX, theY)\n",
    "    importances = clf.feature_importances_\n",
    "    print importances\n",
    "    print importances.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some draft code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data for gradient boosting    ?????????????????????\n",
    "import pandas\n",
    "def prepare_for_boosting(fileName, isTest):\n",
    "    # Считайте таблицу с признаками из файла features.csv с помощью кода, приведенного выше. \n",
    "\n",
    "    features = pandas.read_csv(fileName, index_col='match_id')\n",
    "    print features.head()\n",
    "    print features.info()\n",
    "    \n",
    "    if (isTest):        \n",
    "        match_info = features\n",
    "    else:    \n",
    "        match_info, y = remove_result_columns(features)\n",
    "        \n",
    "    X_removed_heroes_rooms = remove_initial_heroes_and_lobbies(match_info)\n",
    "    X = replace_skpped_values_with_0(X_removed_heroes_rooms)\n",
    "\n",
    "    xxx_heroes= words_bag_of_heroes(match_info)\n",
    "    print type(xxx_heroes)\n",
    "    print xxx_heroes.shape\n",
    "\n",
    "    xxx_rooms= words_bag_of_rooms(match_info)\n",
    "    print type(xxx_rooms)\n",
    "    print xxx_rooms.shape\n",
    "\n",
    "    xxx_rooms_and_heroes=join_bags_rooms_and_heroes(xxx_heroes , xxx_rooms)\n",
    "\n",
    "    X_bagged = add_word_bag(X,xxx_rooms_and_heroes)\n",
    "    if (isTest):        \n",
    "        return X_bagged, X\n",
    "    else:    \n",
    "        return X_bagged, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Skip it ++++++++++++++++++++++\n",
    "# Считаем для 30 - max_depth=6\n",
    "rez_scores_local = list()\n",
    "for n_estimators in [1,2]:    \n",
    "    rez_scores_local.append((n_estimators, calculate_scores_for_estimators(n_estimators, x_bust , y_bust, max_depth=1)))    \n",
    "rez_scores_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if max score is good\n",
    "import sys\n",
    "print type(rez_scores_local)\n",
    "type(rez_scores_local[0])\n",
    "max_score=-sys.maxint - 1\n",
    "max_index=\"a\"\n",
    "for index in range(len(rez_scores_local)):\n",
    "    if (rez_scores_local[index][1][0]>max_score):\n",
    "        max_index=index\n",
    "    print rez_scores_local[index][1][0]\n",
    "print max_score, max_index\n",
    "\n",
    "#   print 'Current fruit :', fruits[index]\n",
    "# rez_scores_local[:]\n",
    "#[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "x_t, y_t, x_data_frame  = prepare_for_log_regr('/Volumes/fast64/My/MachineLearning/week07/Assignment01/data/features.csv', False)\n",
    "\n",
    "\n",
    "xxxx_dataframe = replace_skpped_values_with_0(x_data_frame)\n",
    "clf = DecisionTreeClassifier(random_state=241)\n",
    "clf.fit(xxxx_dataframe , y_t)\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "for index in range(len(importances)):\n",
    "    print pearsonr(xxxx_dataframe[xxxx_dataframe.columns[index]] , y_t), xxxx_dataframe.columns[index] \n",
    "\n",
    "\n",
    "for index in range(len(importances)):\n",
    "    print importances[index] , xxxx_dataframe.columns[index] \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_data_frame[['first_blood_time_team','first_blood_time','first_blood_team']]\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Start' , printtime() \n",
    "x_train, y_train, x_train_data_frame,  = prepare_for_log_regr('/Volumes/fast64/My/MachineLearning/week07/Assignment01/data/features.csv', False)\n",
    "ss = stdScaling_fit(x_train)\n",
    "print 'Done1'\n",
    "X_train_std_scal = stdScaling_transform( x_train ,ss)\n",
    "print 'Done2'\n",
    "\n",
    "log_reg_best_param = calculate_logreg_best_param(X_train_std_scal, y_train)\n",
    "print 'Done3'\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', random_state=555, C=log_reg_best_param['C']) \n",
    "print 'Done4'\n",
    "\n",
    "clf.fit(X_train_std_scal, y_train)\n",
    "\n",
    "print 'Done5'\n",
    "rez_dataframe = store_predicted_probability(clf, X_train_std_scal, x_train_data_frame, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/logreg_train_01_02_03_pow2.csv')\n",
    "print 'Done6' , printtime() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## x_train, y_train, x_train_data_frame  = prepare_for_log_regr('/Volumes/fast64/My/MachineLearning/week07/Assignment01/data/features.csv', False)\n",
    "ss = stdScaling_fit(x_train)\n",
    "print 'Done1'\n",
    "X_train_std_scal = stdScaling_transform( x_train ,ss)\n",
    "print 'Done2'\n",
    "\n",
    "log_reg_best_param = calculate_logreg_best_param(X_train_std_scal, y_train)\n",
    "print 'Done3'\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', random_state=555, C=log_reg_best_param['C']) # !! replace with log_reg_best_param\n",
    "print 'Done4'\n",
    "\n",
    "clf.fit(X_train_std_scal, y_train)\n",
    "\n",
    "print 'Done5'\n",
    "rez_dataframe = store_predicted_probability(clf, X_train_std_scal, x_train_data_frame, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/logreg_train.csv')\n",
    "print 'Done6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_reg_best_param = calculate_logreg_best_param(X_std_scal)\n",
    "log_reg_best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, X_test_data_frame = prepare_for_log_regr('/Volumes/fast64/My/MachineLearning/week07/Assignment01/data/features_test.csv', True)\n",
    "X_test_std_scal = stdScaling_transform( X_test ,ss)\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rez_dataframe = store_predicted_probability(clf, X_test_std_scal, X_test_data_frame, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/logreg_test_01_02_03.csv')\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_train_test(par_estimators, par_max_depth):\n",
    "    # training\n",
    "    print '############## gradient boosting: trainig. Estimators= ' , par_estimators[0], 'max_depth=',  par_max_depth \n",
    "    out_train_file='C:/My/Dota2/03/grad_boost_train_'+str(par_estimators[0])+'_'+str(par_max_depth)+'.csv'\n",
    "    out_test_file='C:/My/Dota2/03/grad_boost_test_'+str(par_estimators[0])+'_'+str(par_max_depth)+'.csv'    \n",
    "    print '---Start train' , printtime() \n",
    "    # x_train_bust, y_train_bust, x_train_data_frame_bust  = prepare_for_gradient_boosting('C:/My/Dota2/Data/features.csv', False)\n",
    "    # print 'Done1'\n",
    "\n",
    "    rez_scores = crossvalidate_estimators(par_estimators, x_train_bust, y_train_bust, max_depth=par_max_depth)\n",
    "    print 'Done2', printtime()   \n",
    "\n",
    "    clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "    clf.fit(x_train_bust, y_train_bust)\n",
    "    print 'Done3', printtime()   \n",
    "\n",
    "    rez_dataframe = store_predicted_probability(clf, x_train_bust, x_train_data_frame_bust, out_train_file)\n",
    "    print 'Done4', printtime()\n",
    "    # test\n",
    "    print '--- Start test ', printtime()  \n",
    "    \n",
    "    #x_test, X_test_data_frame = prepare_for_gradient_boosting('C:/My/Dota2/Data/features_test.csv', True)\n",
    "\n",
    "    rez_dataframe = store_predicted_probability(clf, x_test, X_test_data_frame, out_test_file)\n",
    "\n",
    "    print 'Done', printtime()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start _ all 2016-04-29 04:59:00.062000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (Before). parDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (After). rezDataFrame (97230, 193)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 193 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(110)\n",
      "memory usage: 143.9 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 193)\n",
      "remove_result_columns (After): shape (97230, 187)\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 18008476\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 18182010\n",
      "--Done prepare train 2016-04-29 05:03:27.581000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (Before). parDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (After). rezDataFrame (17177, 187)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 187 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(104)\n",
      "memory usage: 24.6 MB\n",
      "None\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 3181233\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 3212099\n",
      "--Done prepare test 2016-04-29 05:04:15.500000\n",
      "############## gradient boosting: trainig. Estimators=  120 max_depth= 4\n",
      "---Start train 2016-04-29 05:04:15.500000\n",
      "crossvalidate_estimators: start  2016-04-29 05:04:15.500000\n",
      "calculate_scores_for_estimators: start  2016-04-29 05:04:15.500000\n",
      "score_mean= 0.722611854863 scores_std= 0.00355157695532 n_estimators = 120 max_depth= 4 Time: 0:08:36.635000\n",
      "calculate_scores_for_estimators: finish  2016-04-29 05:12:52.135000\n",
      "crossvalidate_estimators: finish  2016-04-29 05:12:52.136000\n",
      "Done2 2016-04-29 05:12:52.136000\n",
      "<type 'list'>\n",
      "0.722611854863\n",
      "max_score = 0.722611854863 max_index= 0 best_n_estimators= 120 best_max_depth= 4\n",
      "Done3 2016-04-29 05:19:40.853000\n",
      "Минимум предсказанных вероятностей = 0.0386755891363\n",
      "Максимум предсказанных вероятностей = 0.973255188784\n",
      "Done4 2016-04-29 05:19:43.114000\n",
      "--- Start test  2016-04-29 05:19:43.115000\n",
      "Минимум предсказанных вероятностей = 0.051422328781\n",
      "Максимум предсказанных вероятностей = 0.967055701536\n",
      "Done 2016-04-29 05:19:43.532000\n",
      "############## gradient boosting: trainig. Estimators=  120 max_depth= 5\n",
      "---Start train 2016-04-29 05:19:43.533000\n",
      "crossvalidate_estimators: start  2016-04-29 05:19:43.533000\n",
      "calculate_scores_for_estimators: start  2016-04-29 05:19:43.533000\n",
      "score_mean= 0.723343393486 scores_std= 0.00336737407191 n_estimators = 120 max_depth= 5 Time: 0:13:19.358000\n",
      "calculate_scores_for_estimators: finish  2016-04-29 05:33:02.891000\n",
      "crossvalidate_estimators: finish  2016-04-29 05:33:02.891000\n",
      "Done2 2016-04-29 05:33:02.891000\n",
      "<type 'list'>\n",
      "0.723343393486\n",
      "max_score = 0.723343393486 max_index= 0 best_n_estimators= 120 best_max_depth= 5\n",
      "Done3 2016-04-29 05:43:24.540000\n",
      "Минимум предсказанных вероятностей = 0.027098937179\n",
      "Максимум предсказанных вероятностей = 0.975209695175\n",
      "Done4 2016-04-29 05:43:27.146000\n",
      "--- Start test  2016-04-29 05:43:27.146000\n",
      "Минимум предсказанных вероятностей = 0.0315088871407\n",
      "Максимум предсказанных вероятностей = 0.965760126901\n",
      "Done 2016-04-29 05:43:27.627000\n",
      "############## gradient boosting: trainig. Estimators=  240 max_depth= 6\n",
      "---Start train 2016-04-29 05:43:27.627000\n",
      "crossvalidate_estimators: start  2016-04-29 05:43:27.627000\n",
      "calculate_scores_for_estimators: start  2016-04-29 05:43:27.627000\n",
      "score_mean= 0.722157014573 scores_std= 0.00353777407016 n_estimators = 240 max_depth= 6 Time: 0:35:36.255000\n",
      "calculate_scores_for_estimators: finish  2016-04-29 06:19:03.882000\n",
      "crossvalidate_estimators: finish  2016-04-29 06:19:03.882000\n",
      "Done2 2016-04-29 06:19:03.882000\n",
      "<type 'list'>\n",
      "0.722157014573\n",
      "max_score = 0.722157014573 max_index= 0 best_n_estimators= 240 best_max_depth= 6\n",
      "Done3 2016-04-29 06:49:34.711000\n",
      "Минимум предсказанных вероятностей = 0.0116367076351\n",
      "Максимум предсказанных вероятностей = 0.990591863657\n",
      "Done4 2016-04-29 06:49:39.282000\n",
      "--- Start test  2016-04-29 06:49:39.283000\n",
      "Минимум предсказанных вероятностей = 0.0184984766359\n",
      "Максимум предсказанных вероятностей = 0.983757106005\n",
      "Done 2016-04-29 06:49:40.087000\n",
      "############## gradient boosting: trainig. Estimators=  480 max_depth= 6\n",
      "---Start train 2016-04-29 06:49:40.088000\n",
      "crossvalidate_estimators: start  2016-04-29 06:49:40.088000\n",
      "calculate_scores_for_estimators: start  2016-04-29 06:49:40.088000\n",
      "score_mean= 0.720117263451 scores_std= 0.00350738293623 n_estimators = 480 max_depth= 6 Time: 1:08:35.689000\n",
      "calculate_scores_for_estimators: finish  2016-04-29 07:58:15.777000\n",
      "crossvalidate_estimators: finish  2016-04-29 07:58:15.778000\n",
      "Done2 2016-04-29 07:58:15.778000\n",
      "<type 'list'>\n",
      "0.720117263451\n",
      "max_score = 0.720117263451 max_index= 0 best_n_estimators= 480 best_max_depth= 6\n",
      "Done3 2016-04-29 08:58:40.669000\n",
      "Минимум предсказанных вероятностей = 0.00255963300789\n",
      "Максимум предсказанных вероятностей = 0.99605469577\n",
      "Done4 2016-04-29 08:58:48.783000\n",
      "--- Start test  2016-04-29 08:58:48.784000\n",
      "Минимум предсказанных вероятностей = 0.00907918324892\n",
      "Максимум предсказанных вероятностей = 0.994931100211\n",
      "Done 2016-04-29 08:58:50.316000\n",
      "############## gradient boosting: trainig. Estimators=  800 max_depth= 6\n",
      "---Start train 2016-04-29 08:58:50.317000\n",
      "crossvalidate_estimators: start  2016-04-29 08:58:50.317000\n",
      "calculate_scores_for_estimators: start  2016-04-29 08:58:50.317000\n",
      "score_mean= 0.716943132097 scores_std= 0.00318958343773 n_estimators = 800 max_depth= 6 Time: 1:53:29.133000\n",
      "calculate_scores_for_estimators: finish  2016-04-29 10:52:19.450000\n",
      "crossvalidate_estimators: finish  2016-04-29 10:52:19.450000\n",
      "Done2 2016-04-29 10:52:19.450000\n",
      "<type 'list'>\n",
      "0.716943132097\n",
      "max_score = 0.716943132097 max_index= 0 best_n_estimators= 800 best_max_depth= 6\n",
      "Done3 2016-04-29 12:31:08.988000\n",
      "Минимум предсказанных вероятностей = 0.000377571297432\n",
      "Максимум предсказанных вероятностей = 0.998753013408\n",
      "Done4 2016-04-29 12:31:22.202000\n",
      "--- Start test  2016-04-29 12:31:22.203000\n",
      "Минимум предсказанных вероятностей = 0.00676655365776\n",
      "Максимум предсказанных вероятностей = 0.997671231018\n",
      "Done 2016-04-29 12:31:24.576000\n",
      "Done _ all 2016-04-29 12:31:24.578000\n"
     ]
    }
   ],
   "source": [
    "print 'Start _ all', printtime()  \n",
    "x_train_bust, y_train_bust, x_train_data_frame_bust  = prepare_for_gradient_boosting('C:/My/Dota2/Data/features.csv', False)\n",
    "print '--Done prepare train', printtime()  \n",
    "x_test, X_test_data_frame = prepare_for_gradient_boosting('C:/My/Dota2/Data/features_test.csv', True)\n",
    "print '--Done prepare test', printtime()  \n",
    "    \n",
    "    \n",
    "gradient_boosting_train_test([120], 4)\n",
    "gradient_boosting_train_test([120], 5)\n",
    "gradient_boosting_train_test([240], 6)\n",
    "gradient_boosting_train_test([480], 6)\n",
    "gradient_boosting_train_test([800], 6)\n",
    "\n",
    "print 'Done _ all', printtime()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг - train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Start' , printtime() \n",
    "x_train_bust, y_train_bust, x_train_data_frame_bust  = prepare_for_gradient_boosting('/Volumes/fast64/My/MachineLearning/week07/Assignment01/data/features.csv', False)\n",
    "print 'Done1'\n",
    "\n",
    "rez_scores = crossvalidate_estimators([50], x_train_bust, y_train_bust, max_depth=6)\n",
    "print 'Done2', printtime()   \n",
    "\n",
    "clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "clf.fit(x_train_bust, y_train_bust)\n",
    "print 'Done3', printtime()   \n",
    "\n",
    "rez_dataframe = store_predicted_probability(clf, x_train_bust, x_train_data_frame_bust, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/grad_boost_train_200_6.csv')\n",
    "print 'Done4', printtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_bust , y_bust, x_bust_dataframe = prepare_for_log_regr('/Volumes/fast64/My/MachineLearning/week07/Assignment01/data/features.csv', False)\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Считаем для 200 - max_depth=6\n",
    "rez_scores_6 = list()\n",
    "for n_estimators in [200]:    \n",
    "    rez_scores_6.append((n_estimators, calculate_scores_for_estimators(n_estimators, x_bust , y_bust, max_depth=6)))    \n",
    "rez_scores_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rez_scores = crossvalidate_estimators([300], x_bust, y_bust, max_depth=8)\n",
    "clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "clf.fit(x_bust, y_bust)\n",
    "rez_dataframe = store_predicted_probability(clf, x_bust, match_info, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/grad_bust_test_300_8.csv')\n",
    "print 'Done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rez_scores = crossvalidate_estimators([200], x_bust, y_bust, max_depth=6)\n",
    "clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "#clf.fit(x_bust, y_bust)\n",
    "#rez_dataframe = store_predicted_probability(clf, x_bust, x_bust_dataframe, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/grad_bust_train_200_6.csv')\n",
    "print 'Done', printtime()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rez_dataframe = store_predicted_probability(clf, x_bust, x_bust_dataframe, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/grad_bust_train_200_6.csv')\n",
    "print 'Done', printtime()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rez_dataframe = store_predicted_probability(clf, x_bust, x_bust_dataframe, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/grad_bust_train_200_6.csv')\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг - test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Start', printtime()  \n",
    "x_test, X_test_data_frame = prepare_for_gradient_boosting('/Volumes/fast64/My/MachineLearning/week07/Assignment01/data/features_test.csv', True)\n",
    "\n",
    "rez_dataframe = store_predicted_probability(clf, x_test, X_test_data_frame, '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/grad_bust_test_200_6.csv')\n",
    "print 'Done', printtime()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сливаем результаты в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# сливаем массив результирующих csv в один\n",
    "\n",
    "def join_all_results(files, rez_file_name):\n",
    "    src_list= list()\n",
    "    for fl in files:    \n",
    "        fl_content = pandas.read_csv(fl, index_col='match_id')\n",
    "        print fl_content.shape\n",
    "        src_list.append(fl_content)\n",
    "\n",
    "    lngth = len(src_list[0].index)\n",
    "    rez = [0] * lngth\n",
    "\n",
    "    src_num = len(src_list)\n",
    "\n",
    "    for sss in src_list:\n",
    "        rez = rez + sss['radiant_win']\n",
    "\n",
    "    rez =rez/src_num\n",
    "\n",
    "    test_check = {\n",
    "        'match_id': src_list[0].index, \n",
    "        'radiant_win': rez\n",
    "    }\n",
    "\n",
    "    test_check_dframe = pandas.DataFrame.from_dict(test_check)\n",
    "    test_check_dframe.set_index('match_id')\n",
    "\n",
    "    test_check_dframe.to_csv(rez_file_name, index=False, columns=['match_id', 'radiant_win'])\n",
    "    print 'done join_all_results'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ['/Volumes/fast64/My/MachineLearning/week07/Dota2_03/grad_bust_test_300_8.csv', \n",
    "        '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/logreg_test.csv']\n",
    "rez_file_name = '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/summary_test1.csv'\n",
    "join_all_results(files, rez_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Старое  --------\n",
    "# Старое  -------- \n",
    "# Старое  -------- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подход 1: градиентный бустинг \"в лоб\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Проверьте выборку на наличие пропусков с помощью функции count(), которая для каждого столбца показывает число заполненных значений. Много ли пропусков в данных? Запишите названия признаков, имеющих пропуски, и попробуйте для любых двух из них дать обоснование, почему их значения могут быть пропущены.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_info_count = match_info.count()\n",
    "missing_data_columns_info = match_info_count[match_info_count != match_info_count.max()]\n",
    "\n",
    "print \"Q: Много ли пропусков в данных?\"\n",
    "print \"A: \",len(missing_data_columns_info),\"признаков имеют пропуски в данных?\",\"\\n\"\n",
    "\n",
    "print  \"названия признаков, имеющих пропуски\"\n",
    "\n",
    "\n",
    "print missing_data_columns_info\n",
    "# !!!!!!!!!!! посчитать % пропуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Q: попробуйте для любых двух из них дать обоснование, почему их значения могут быть пропущены <br/>\n",
    "A:  <br/>\n",
    "поля <b>first_blood_time, first_blood_team, first_blood_player1</b><br/>\n",
    "не было первой крови в первые 5 минут\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# проверим, что нет признаков с пропусками\n",
    "X_count=X.count()\n",
    "ZZZ = X_count[X_count != X_count.max()]\n",
    "print \"Количество признаков с пропусками после заполнения =\", ZZZ.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Какой столбец содержит целевую переменную? Запишите его название.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Q: Какой столбец содержит целевую переменную? Запишите его название. <br/>\n",
    "A:  <br/>\n",
    "столбец <b>radiant_win</b><br/>\n",
    "1, если победила команда Radiant, 0 — иначе\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Забудем, что в выборке есть категориальные признаки, и попробуем обучить градиентный бустинг над деревьями на имеющейся матрице \"объекты-признаки\". Зафиксируйте генератор разбиений для кросс-валидации по 5 блокам (KFold), не забудьте перемешать при этом выборку (shuffle=True), поскольку данные в таблице отсортированы по времени, и без перемешивания можно столкнуться с нежелательными эффектами при оценивании качества. Оцените качество градиентного бустинга (GradientBoostingClassifier) с помощью данной кросс-валидации, попробуйте при этом разное количество деревьев (как минимум протестируйте следующие значения для количества деревьев: 10, 20, 30). Долго ли настраивались классификаторы? Достигнут ли оптимум на испытанных значениях параметра n_estimators, или же качество, скорее всего, продолжит расти при дальнейшем его увеличении?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "import datetime\n",
    "\n",
    "def calculate_scores_for_estimators(n_estimators, max_depth=3):\n",
    "    start_time = datetime.datetime.now()\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "        random_state=555, max_depth=max_depth,\n",
    "    )\n",
    "    k_fold = KFold(len(y), n_folds=5, random_state=555, shuffle=True)\n",
    "    local_scores = cross_val_score(clf, X, y, cv=k_fold, scoring='roc_auc')\n",
    "    print 'n_estimators =', n_estimators, 'max_depth=', max_depth, 'Time elapsed:', datetime.datetime.now() - start_time\n",
    "    return local_scores.mean(), local_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Считаем для 10,20,30, 100, 200 - max_depth=3\n",
    "rez_scores_3 = list()\n",
    "for n_estimators in [10,20,30,100, 200]:    \n",
    "    rez_scores_3.append((n_estimators, calculate_scores_for_estimators(n_estimators=n_estimators)))    \n",
    "rez_scores_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Считаем для 10,20,30 - max_depth=2\n",
    "rez_scores_2 = list()\n",
    "for n_estimators in [10,20,30]:    \n",
    "    rez_scores_2.append((n_estimators, calculate_scores_for_estimators(n_estimators=n_estimators, max_depth=2)))    \n",
    "rez_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Считаем для 10,20,30 - max_depth=6\n",
    "rez_scores_6 = list()\n",
    "for n_estimators in [10,20,30]:    \n",
    "    rez_scores_6.append((n_estimators, calculate_scores_for_estimators(n_estimators=n_estimators, max_depth=6)))    \n",
    "rez_scores_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подход 2: логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "1. Оцените качество логистической регрессии (sklearn.linear_model.LogisticRegression с L2-регуляризацией) с помощью кросс-валидации по той же схеме, которая использовалась для градиентного бустинга. Подберите при этом лучший параметр регуляризации (C). Какое наилучшее качество у вас получилось? Как оно соотносится с качеством градиентного бустинга? Чем вы можете объяснить эту разницу? Быстрее ли работает логистическая регрессия по сравнению с градиентным бустингом?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calculate_logreg(X_std_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "4. Воспользуемся подходом \"мешок слов\" для кодирования информации о героях. Пусть всего в игре имеет N различных героев. Сформируем N признаков, при этом i-й будет равен нулю, если i-й герой не участвовал в матче; единице, если i-й герой играл за команду Radiant; минус единице, если i-й герой играл за команду Dire. Ниже вы можете найти код, который выполняет данной преобразование. Добавьте полученные признаки к числовым, которые вы использовали во втором пункте данного этапа.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pick = np.zeros((X.shape[0], max(unique_heroes)))\n",
    "\n",
    "for i, match_id in enumerate(X.index):\n",
    "    for p in xrange(5):\n",
    "        X_pick[i, X.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "        X_pick[i, X.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_pick.shape, type(X_pick.shape)\n",
    "print X_categor_removed.shape, type(X_categor_removed)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_categor_removed_bag_words = hstack([X_categor_removed, X_pick]).toarray()\n",
    "print X_categor_removed_bag_words.shape, type(X_categor_removed_bag_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "5. Проведите кросс-валидацию для логистической регрессии на новой выборке с подбором лучшего параметра регуляризации. Какое получилось качество? Улучшилось ли оно? Чем вы можете это объяснить?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_categor_removed_bag_words_std_scal = StandardScaler().fit_transform(X_categor_removed_bag_words)\n",
    "calculate_logreg(X_categor_removed_bag_words_std_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# добавим мешок слов по комнате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "unique_lobbyes = np.unique(X['lobby_type'])\n",
    "print unique_lobbyes\n",
    "print 'unique_lobbyes.size=', unique_lobbyes.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''id,name\n",
    "-1,Invalid\n",
    "0,Public matchmaking\n",
    "1,Practice\n",
    "2,Tournament\n",
    "3,Tutorial\n",
    "4,Co-op with bots\n",
    "5,Team match\n",
    "6,Solo Queue\n",
    "7,Ranked\n",
    "8,Solo Mid 1vs1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rooms = 10\n",
    "X_pick_room = np.zeros([X.shape[0], num_rooms])   # !!! two (())\n",
    "\n",
    "for i, match_id in enumerate(X.index):\n",
    "    for p in xrange(10):\n",
    "        X_pick_room[i, X.ix[match_id, 'lobby_type']+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print X_pick.shape\n",
    "print X_pick_room.shape\n",
    "X_pick_heroes_room = np.concatenate((X_pick , X_pick_room),  axis=1)\n",
    "print X_pick_heroes_room.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_pick_heroes_room.shape, type(X_pick_heroes_room.shape)\n",
    "print X_categor_removed.shape, type(X_categor_removed)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_categor_removed_bag_words_hero_room = hstack([X_categor_removed, X_pick_heroes_room]).toarray()\n",
    "print X_categor_removed_bag_words_hero_room.shape, type(X_categor_removed_bag_words_hero_room)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler().fit(X_categor_removed_bag_words_hero_room)\n",
    "ttt= ss.transform(X_categor_removed_bag_words_hero_room)\n",
    "print ttt.mean(), ttt.min(), ttt.max()\n",
    "X_categor_removed_bag_words_hero_room_std_scal = StandardScaler().fit_transform(X_categor_removed_bag_words_hero_room)\n",
    "print X_categor_removed_bag_words_hero_room_std_scal.mean(), X_categor_removed_bag_words_hero_room_std_scal.min(), X_categor_removed_bag_words_hero_room_std_scal.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_categor_removed_bag_words_hero_room_std_scal = StandardScaler().fit_transform(X_categor_removed_bag_words_hero_room)\n",
    "calculate_logreg(X_categor_removed_bag_words_hero_room_std_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "6. Постройте предсказания вероятностей победы команды Radiant для тестовой выборки с помощью лучшей из изученных моделей (лучшей с точки зрения AUC-ROC на кросс-валидации). Убедитесь, что предсказанные вероятности адекватные — находятся на отрезке [0, 1], не совпадают между собой (т.е. что модель не получилась константной).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_test = pandas.read_csv('/Volumes/fast64/My/MachineLearning/week07/Assignment01/data/features_test.csv', index_col='match_id')\n",
    "print features_test.info()\n",
    "X_test = features_test.fillna(0)\n",
    "\n",
    "X_test_categor_removed= X_test.drop( heros+ ['lobby_type'], axis=1)\n",
    "\n",
    "test_unique_heroes = np.unique(X_test[heros])\n",
    "print test_unique_heroes\n",
    "print 'test_unique_heroes.size=', test_unique_heroes.size\n",
    "\n",
    "\n",
    "# X_test_categor_removed_std_scal = StandardScaler().fit_transform(X_test_categor_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pick = np.zeros((X_test.shape[0], max(test_unique_heroes)))\n",
    "\n",
    "for i, match_id in enumerate(X_test.index):\n",
    "    for p in xrange(5):\n",
    "        X_pick[i, X_test.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "        X_pick[i, X_test.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rooms = 10\n",
    "X_pick_room = np.zeros([X_test.shape[0], num_rooms])   # !!! two (())\n",
    "\n",
    "for i, match_id in enumerate(X_test.index):\n",
    "    for p in xrange(10):\n",
    "        X_pick_room[i, X_test.ix[match_id, 'lobby_type']+1] = 1\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_test_pick_heroes_room = np.concatenate((X_pick , X_pick_room),  axis=1)\n",
    "\n",
    "print X_test_pick_heroes_room.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_test_pick_heroes_room.shape\n",
    "print X_test_categor_removed.shape\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_test_categor_removed_bag_words = hstack([X_test_categor_removed, X_test_pick_heroes_room]).toarray()\n",
    "print X_test_categor_removed_bag_words.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_categor_removed_bag_words_std_scal = ss.transform(X_test_categor_removed_bag_words)\n",
    "print X_test_categor_removed_bag_words_std_scal.mean(), X_test_categor_removed_bag_words_std_scal.min(), X_test_categor_removed_bag_words_std_scal.max()\n",
    "\n",
    "ttt = StandardScaler().fit_transform(X_categor_removed_bag_words_hero_room)\n",
    "print ttt.mean(), ttt.min(), ttt.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', random_state=555, C=0.005)\n",
    "clf.fit(X_categor_removed_bag_words_hero_room_std_scal, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# smoke check\n",
    "rez=clf.predict_proba(X_categor_removed_bag_words_hero_room_std_scal)[:, 1]\n",
    "\n",
    "smoke_check = {\n",
    "    'match_id': X.index, \n",
    "    'radiant_win_prob': rez,\n",
    "    'radiant_win_actual': y[X.index]\n",
    "}\n",
    "\n",
    "smoke_check_dframe = pandas.DataFrame.from_dict(smoke_check)\n",
    "\n",
    "smoke_check_dframe.set_index('match_id')\n",
    "\n",
    "num_good_predictions=0\n",
    "num_bad_predictions=0\n",
    "for index, row in smoke_check_dframe.iterrows():\n",
    "    if (row.radiant_win_prob >= 0.5 and row.radiant_win_actual == 1 or \n",
    "       row.radiant_win_prob < 0.5 and row.radiant_win_actual == 0):\n",
    "        num_good_predictions=num_good_predictions+1\n",
    "    else:\n",
    "        num_bad_predictions=num_bad_predictions+1\n",
    "print 'num_good_predictions=', num_good_predictions, 'num_bad_predictions=', num_bad_predictions\n",
    "print 'ratio=', 100.0*num_good_predictions/(num_good_predictions+num_bad_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rez=clf.predict_proba(X_test_categor_removed_bag_words_std_scal)[:, 1]\n",
    "\n",
    "print \"Минимум предсказанных вероятностей =\",min(rez)\n",
    "print \"Максимум предсказанных вероятностей =\", max(rez)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_check = {\n",
    "    'match_id': X_test.index, \n",
    "    'radiant_win': rez\n",
    "}\n",
    "\n",
    "test_check_dframe = pandas.DataFrame.from_dict(test_check)\n",
    "test_check_dframe.set_index('match_id')\n",
    "\n",
    "test_check_dframe.to_csv('/Volumes/fast64/My/MachineLearning/week07/Dota2_01/dota2-kaggle-01.csv', index=False, columns=['match_id', 'radiant_win'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
