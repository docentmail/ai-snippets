{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization reservs\n",
    "- average for градиенный бустинг и линейную регрессию\n",
    "- убрать время начала и номер матча\n",
    "- сделать чтобы все матчи начинались с 0. Без отрицательных времен\n",
    "- продумать с пустыми значениями - их очень много\n",
    "- (Done 01 - use it) first_blood_time', 'first_blood_team' - алгоритм понимает что это связка???? только время - (+) для одной команды и (-) для другой   - Only worse!!!\n",
    "- посчитать \"производительность\" команд каждую минуту. Может динамика поможет. \n",
    "- покуртить ручки алгоритмов - может поможет\n",
    "- + добавить степени числовых признаков - выйти за линейность - **2 для градиентного бустинга\n",
    "- сколько -1 комнат в списке\n",
    "- (Done modify_dayaframe_03 - use it) попробовать просуммировать однотипные поля команды\n",
    "\n",
    "Development convinience\n",
    " - small subset to test that code does not throw exceptions  - done\n",
    " - reasonable file naming \n",
    " - self documented results\n",
    " - one button run\n",
    " \n",
    "#### Done\n",
    "Version 04\n",
    "- use Estimator output as additional column for other estimator\n",
    "\n",
    "\n",
    "Version 03\n",
    "- good data prepararion functions for LogReg and Grad Boosting\n",
    "- Run multiple gradient bustings together (night run)\n",
    "- implement run on small data subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def add_random_column_to_datafram(parDataFrame, column_name):\n",
    "    ''' adds column with [0:1) values\n",
    "    '''\n",
    "    parDataFrame[column_name] = \\\n",
    "        parDataFrame.apply(lambda row: np.random.random() , axis=1)\n",
    "    \n",
    "    \n",
    "# create dataframe\n",
    "d = {'one' : [1., 2., 3., 4.], 'two' : [4., 3., 2., 1.], 'three' : [7., 8., 9., 10.]}\n",
    "df= pd.DataFrame(d, index=[0, 2,  1, 3 ])\n",
    "print df\n",
    "print df.index\n",
    "    \n",
    "add_random_column_to_datafram(df, 'rnd')    \n",
    "print df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print np.random.random()\n",
    "print np.random.random()\n",
    "print np.random.random()\n",
    "print np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# merge datasets experements\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "raw_data = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5'],\n",
    "        'first_name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], \n",
    "        'last_name': ['Anderson', 'Ackerman', 'Ali', 'Aoni', 'Atiches']}\n",
    "df_a = pd.DataFrame(raw_data, columns = ['subject_id', 'first_name', 'last_name'])\n",
    "df_a.index.name='match_id'\n",
    "\n",
    "print 'df_a', df_a.shape\n",
    "print '======'\n",
    "print df_a.head()\n",
    "print '======'\n",
    "\n",
    "raw_data_1 = {\n",
    "        'title': ['Mr', 'Ms', 'Ms', 'Mr', 'Ms', 'aa']}\n",
    "df_b = pd.DataFrame(raw_data_1, columns = ['title'])\n",
    "df_b.index.name='match_id'\n",
    "print 'df_b', df_b.shape\n",
    "print '======'\n",
    "print df_b.head()\n",
    "print '======'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "aa = pd.merge(df_a, df_b, right_index=True, left_index=True, how='inner')\n",
    "print 'df_a', df_a.shape\n",
    "print 'df_b', df_b.shape\n",
    "print 'aa', aa.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# on 1000 - best param =  {'C': 0.00080000000000000004} best score =  0.747402682882\n",
    "\n",
    "# configuration variable\n",
    "number_rows_to_use= -1 # 100   # -1 all records\n",
    "feature_train_file_config = '../dota_kaggle_features/features.csv'\n",
    "feature_test_file_config = '../dota_kaggle_features/features_test.csv'\n",
    "\n",
    "\n",
    "# for logistic regression \n",
    "power_value_config=2     # see   modify_dayaframe_02\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrue_file = feature_train_file_config\\npred_file = '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/ExportedRez_log_regeression/logreg_train_01_02_03_pow2.csv'\\ncalculated_frame = prepare_true_pred(pred_file, true_file)\\n\\n\\nprint calculated_frame.mean(axis=0)\\nprint accuracy_score(calculated_frame['radiant_win'], calculated_frame['radiant_win_pred'])\\nprint roc_auc_score(calculated_frame['radiant_win'], calculated_frame['radiant_win_prob'])\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# smoke check - calculate accurancies \n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "\n",
    "def prepare_true_pred(pred_file , true_file=feature_train_file_config):\n",
    "    ''' Prepares dataframe to use for scoring (see sklearn.metrics)\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_file\n",
    "        file like ..../logreg_train_01_02_03.csv ('match_id' - index , 'radiant_win' column)\n",
    "    \n",
    "    features.csv\n",
    "        file ..../features.csv \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        dataframe with \n",
    "            'match_id' - index ,\n",
    "            'radiant_win' - actual win from foriginal train file\n",
    "            \n",
    "            'radiant_win_prob'- calculated prediction probability (like 0.54366425)\n",
    "            'radiant_win_pred' - predicted class calculated base on 'radiant_win_prob' (0,1)\n",
    "            'good' - right prediction rows: 1 if 'radiant_win_pred'=='radiant_win, 0 otherwise\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> true_file=feature_train_file_config\n",
    "    >>> pred_file = './Dota2_02/logreg_train_01_02_03.csv'\n",
    "    >>> calculated_frame = prepare_true_pred(pred_file, true_file)\n",
    "    >>> print calculated_frame.mean(axis=0)\n",
    "    >>> print accuracy_score(calculated_frame['radiant_win'], calculated_frame['radiant_win_pred'])\n",
    "\n",
    "    '''\n",
    "    features = pandas.read_csv(true_file, index_col='match_id')\n",
    "    features = features[['radiant_win']]\n",
    "    features_rez = pandas.read_csv(pred_file, index_col='match_id')\n",
    "    features_rez.columns = ['radiant_win_prob']\n",
    "    # merged_dataset= pd.merge(features, features_rez, right_index=True, left_index=True, how='inner')\n",
    "    merged_dataset= merge_dataframes_preserve_order(features, features_rez)\n",
    "\n",
    "    merged_dataset['good'] = merged_dataset.apply(lambda row: row['radiant_win']* int(row['radiant_win_prob']>0.5)+ \\\n",
    "        (1-row['radiant_win'])* int(row['radiant_win_prob']<0.5), axis=1)\n",
    "    merged_dataset['radiant_win_pred'] = merged_dataset.apply(lambda row: int(row['radiant_win_prob']>0.5), axis=1)\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "'''\n",
    "true_file = feature_train_file_config\n",
    "pred_file = '/Volumes/fast64/My/MachineLearning/week07/Dota2_03/ExportedRez_log_regeression/logreg_train_01_02_03_pow2.csv'\n",
    "calculated_frame = prepare_true_pred(pred_file, true_file)\n",
    "\n",
    "\n",
    "print calculated_frame.mean(axis=0)\n",
    "print accuracy_score(calculated_frame['radiant_win'], calculated_frame['radiant_win_pred'])\n",
    "print roc_auc_score(calculated_frame['radiant_win'], calculated_frame['radiant_win_prob'])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# create nparrayt\\nnp_array = np.array([[1, 2], [4, 5], [4, 5], [6, 7]], np.int32)\\nprint type(np_array), np_array.shape\\nprint np_array\\n\\n\\n# create dataframe\\nd = {'one' : [1., 2., 3., 4.], 'two' : [4., 3., 2., 1.], 'three' : [7., 8., 9., 10.]}\\ndf= pd.DataFrame(d, index=[0, 2,  1, 3 ])\\nprint df\\nprint df.index\\n\\n# test \\ndf_rez= add_nparray_to_dataframe(df, np_array, 'added')\\nprint df_rez\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding ndarray to dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_nparray_to_dataframe(df, np_arr, col_pref):\n",
    "    '''appends each row of dataframe with row of nparray\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        dataframe - rows of which would be appended with\n",
    "    \n",
    "    np_arr\n",
    "        nparray that rows would be appendedn to dataframe\n",
    "    \n",
    "    col_pref\n",
    "        preffix for added from np_arr columns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        dataframe \n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> df_rez= add_nparray_to_dataframe(df, np_array, 'added')\n",
    "    '''\n",
    "    # check shape\n",
    "    if (df.shape[0] <> np_arr.shape[0]):\n",
    "        raise NameError('number of rows is nor equal: \\n'+ \n",
    "                        'in dataframe is '+ str(df.shape[0])+ '\\n'+\n",
    "                        'in nparray is '+ str(np_arr.shape[0])\n",
    "                       )  \n",
    "\n",
    "    # create array for colum names\n",
    "    columns=[]\n",
    "    for i in xrange(np_arr.shape[1]):\n",
    "        columns.append(col_pref+str(i));\n",
    "    print 'added columns: ', columns    \n",
    "    \n",
    "    # create dataframe from nparray\n",
    "    df1= pd.DataFrame(np_arr, columns=columns, index = df.index)\n",
    "    # merging\n",
    "    # df_rez= pd.merge(df, df1, right_index=True, left_index=True, how='inner')\n",
    "    df_rez= merge_dataframes_preserve_order(df, df1)\n",
    "    return df_rez\n",
    "\n",
    "'''\n",
    "# create nparrayt\n",
    "np_array = np.array([[1, 2], [4, 5], [4, 5], [6, 7]], np.int32)\n",
    "print type(np_array), np_array.shape\n",
    "print np_array\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "d = {'one' : [1., 2., 3., 4.], 'two' : [4., 3., 2., 1.], 'three' : [7., 8., 9., 10.]}\n",
    "df= pd.DataFrame(d, index=[0, 2,  1, 3 ])\n",
    "print df\n",
    "print df.index\n",
    "\n",
    "# test \n",
    "df_rez= add_nparray_to_dataframe(df, np_array, 'added')\n",
    "print df_rez\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nadditional_rez_file='/Volumes/fast64/My/MachineLearning/week07/Dota2_03/ExportedRez_log_regeression/logreg_train_01_02_03_pow4.csv'\\nx_train_bust, y_train_bust, x_train_data_frame_bust  = prepare_for_gradient_boosting(feature_train_file_config, False)\\nprint 'initial x_train_bust[start_time] ', x_train_bust['start_time'].head(5)\\nprint 'initial y_train_bust', y_train_bust.head(5)\\nprint 'initial x_train_data_frame_bust[start_time] ', x_train_data_frame_bust['start_time'].head(5)\\nx_train_bust = add_calculated_prob(additional_rez_file, x_train_bust )\\nprint 'final x_train_bust[start_time] ', x_train_bust['start_time'].head(5)\\nprint 'final y_train_bust', y_train_bust.head(5)\\nprint 'final x_train_data_frame_bust[start_time] ', x_train_data_frame_bust['start_time'].head(5)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "add additional column to dataset\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def merge_dataframes_preserve_order(df_base, df_added):\n",
    "    ''' merges two dataframes with preserving order\n",
    "    wrapper for pd.merge(df_base, df_added, right_index=True, left_index=True, how='inner')\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_base\n",
    "        dataframe that will be used to merge in\n",
    "    \n",
    "    df_added\n",
    "        dataframe that will be used to append \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        dataframe with concatinated rows\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> merged_dataset = merge_dataframes_preserve_order(orig_dataset, features_ext)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    print 'merge_dataframes_preserve_order (Start)'    \n",
    "    name_order_column='Order_949596643'\n",
    "    # to check NANs\n",
    "    df_base_nonzero=np.count_nonzero(np.isnan(df_base))\n",
    "    df_base_zero=np.count_nonzero(~np.isnan(df_base))\n",
    "    # to check shapes\n",
    "    df_base_shape=df_base.shape\n",
    "    df_added_shape=df_added.shape\n",
    "    \n",
    "\n",
    "    # merging and restore order                                 \n",
    "    df_base[name_order_column] = np.arange(len(df_base))\n",
    "    merged_dataset = pd.merge(df_base, df_added, right_index=True, left_index=True, how='inner')\n",
    "    merged_dataset = merged_dataset.sort(name_order_column) \n",
    "    merged_dataset=merged_dataset.drop(name_order_column, 1)\n",
    "    df_base=df_base.drop(name_order_column, 1)\n",
    "    \n",
    "    # checking NANs\n",
    "    if (df_base_nonzero != np.count_nonzero(np.isnan(merged_dataset))):\n",
    "        raise NameError('number of NAN elements was changed: \\n'+ \n",
    "            'in df_base is '+ str(df_base_nonzero)+ '\\n'+\n",
    "            'in merged_dataset is '+ str(np.count_nonzero(np.isnan(merged_dataset)))\n",
    "                        ) \n",
    "\n",
    "    \n",
    "    if (df_base_zero == np.count_nonzero(~np.isnan(merged_dataset))):\n",
    "        raise NameError('number of non NAN elements was NOT changed: \\n'+ \n",
    "            'in df_base is '+ str(df_base_zero)+ '\\n'+\n",
    "            'in merged_dataset is '+ str(np.count_nonzero(~np.isnan(merged_dataset)))\n",
    "                        ) \n",
    "    # check shape\n",
    "    if df_base_shape[0] <> merged_dataset.shape[0] :\n",
    "        raise NameError('number of rows is nor equal: \\n'+ \n",
    "                        'in df_base_shape is '+ str(df_base_shape[0])+ '\\n'+\n",
    "                        'in merged_dataset is '+ str(merged_dataset.shape[0])\n",
    "                       )  \n",
    "    if df_base_shape[1]+df_added_shape[1] <> merged_dataset.shape[1] :\n",
    "        print 'df_base.columns=', df_base.columns\n",
    "        print 'df_added.columns=', df_added.columns\n",
    "        print 'merged_dataset.columns=', merged_dataset.columns\n",
    "        raise NameError('number of columns is nor equal in merged: \\n'+ \n",
    "                        'in df_base_shape[1] is '+ str(df_base_shape[1])+ '\\n'+\n",
    "                        'in df_added_shape[1]  is '+ str(df_added_shape[1] )+ '\\n'+\n",
    "                        'in merged_dataset.shape[1] is '+ str(merged_dataset.shape[1])\n",
    "                       )  \n",
    "    if df_base_shape[0] != df_base.shape[0] or df_base_shape[1] != df_base.shape[1] :\n",
    "        raise NameError('df_base shape was changed: \\n'+ \n",
    "                        'df_base shape before is '+ str(df_base_shape)+ '\\n'+\n",
    "                        'df_base shape after is '+ str(df_base.shape)\n",
    "                       )  \n",
    "    if df_added_shape[0] != df_added.shape[0] or df_added_shape[1] != df_added.shape[1] :\n",
    "        raise NameError('df_added shape was changed: \\n'+ \n",
    "                        'df_added shape before is '+ str(df_added_shape)+ '\\n'+\n",
    "                        'df_added shape after is '+ str(df_added.shape)\n",
    "                       )  \n",
    "\n",
    "\n",
    "  \n",
    "    print 'merge_dataframes_preserve_order (End)'            \n",
    "    return merged_dataset\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# adds aditional column to dataset.\n",
    "# gets this additional column from preliminary created file of results\n",
    "def add_calculated_prob(rez_file_name, orig_dataset ):\n",
    "    print 'add_calculated_prob (Start)'\n",
    "    features_ext = pandas.read_csv(rez_file_name, index_col='match_id')\n",
    "    merged_dataset = merge_dataframes_preserve_order(orig_dataset, features_ext)\n",
    "    print 'add_calculated_prob (End)'    \n",
    "    return merged_dataset\n",
    "\n",
    "'''\n",
    "additional_rez_file='./Dota2_03/ExportedRez_log_regeression/logreg_train_01_02_03_pow4.csv'\n",
    "x_train_bust, y_train_bust, x_train_data_frame_bust  = prepare_for_gradient_boosting(feature_train_file_config, False)\n",
    "print 'initial x_train_bust[start_time] ', x_train_bust['start_time'].head(5)\n",
    "print 'initial y_train_bust', y_train_bust.head(5)\n",
    "print 'initial x_train_data_frame_bust[start_time] ', x_train_data_frame_bust['start_time'].head(5)\n",
    "x_train_bust = add_calculated_prob(additional_rez_file, x_train_bust )\n",
    "print 'final x_train_bust[start_time] ', x_train_bust['start_time'].head(5)\n",
    "print 'final y_train_bust', y_train_bust.head(5)\n",
    "print 'final x_train_data_frame_bust[start_time] ', x_train_data_frame_bust['start_time'].head(5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# annoying warnings disabling\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# global variables\n",
    "heroes = [\n",
    "    'r1_hero',\n",
    "    'r2_hero',\n",
    "    'r3_hero',\n",
    "    'r4_hero',\n",
    "    'r5_hero',\n",
    "    'd1_hero',\n",
    "    'd2_hero',\n",
    "    'd3_hero',\n",
    "    'd4_hero',\n",
    "    'd5_hero'\n",
    "]\n",
    "fields_to_remove = [\n",
    "    'r1_hero',\n",
    "    'r2_hero',\n",
    "    'r3_hero',\n",
    "    'r4_hero',\n",
    "    'r5_hero',\n",
    "    'd1_hero',\n",
    "    'd2_hero',\n",
    "    'd3_hero',\n",
    "    'd4_hero',\n",
    "    'd5_hero',\n",
    "    'lobby_type'\n",
    "#    'start_time'\n",
    "]\n",
    "\n",
    "import datetime\n",
    "\n",
    "def printtime():\n",
    "    return datetime.datetime.utcnow()\n",
    "\n",
    "\n",
    "# Удалите признаки, связанные с итогами матча (они помечены в описании данных как отсутствующие в тестовой выборке).\n",
    "'''\n",
    "Итог матча (данные поля отсутствуют в тестовой выборке, поскольку содержат информацию, выходящую за пределы первых 5 минут матча)\n",
    "duration: длительность\n",
    "radiant_win: 1, если победила команда Radiant, 0 — иначе\n",
    "Состояние башен и барраков к концу матча (см. описание полей набора данных)\n",
    "tower_status_radiant\n",
    "tower_status_dire\n",
    "barracks_status_radiant\n",
    "barracks_status_dire\n",
    "'''\n",
    "\n",
    "# usage: match_info, y = remove_result_columns(features)\n",
    "def remove_result_columns(the_features):\n",
    "    print \"remove_result_columns (Before): shape\",the_features.shape\n",
    "    rez_col_names = [\n",
    "    'duration',\n",
    "    'radiant_win',\n",
    "    'tower_status_radiant',\n",
    "    'tower_status_dire',\n",
    "    'barracks_status_radiant',\n",
    "    'barracks_status_dire'\n",
    "    ]\n",
    "    # res_info = features[rez_col_names]\n",
    "    removed_rez = the_features.drop(rez_col_names, axis=1)\n",
    "    print \"remove_result_columns (After): shape\",removed_rez.shape\n",
    "    return removed_rez, the_features[['radiant_win']]\n",
    "\n",
    "\n",
    "# remove initial heroes and lobbies columns\n",
    "def remove_initial_heroes_and_lobbies(parX):\n",
    "    print parX.shape, type(parX)\n",
    "    X_categor_removed= parX.drop( heroes+ ['lobby_type'], axis=1)\n",
    "    print X_categor_removed.shape, type(X_categor_removed)\n",
    "    return X_categor_removed\n",
    "\n",
    "'''\n",
    "Замените пропуски на нули с помощью функции fillna(). \n",
    "На самом деле этот способ является предпочтительным для логистической регрессии, поскольку он позволит пропущенному \n",
    "значению не вносить никакого вклада в предсказание. Для деревьев часто лучшим вариантом оказывается замена пропуска \n",
    "на очень большое или очень маленькое значение — в этом случае при построении разбиения вершины можно будет отправить \n",
    "объекты с пропусками в отдельную ветвь дерева. Также есть и другие подходы — например, замена пропуска на среднее \n",
    "значение признака. Мы не требуем этого в задании, но при желании попробуйте разные подходы к обработке пропусков и \n",
    "сравните их между собой.\n",
    "'''\n",
    "# Замените пропуски на нули с помощью функции fillna().\n",
    "def replace_skpped_values_with_0(theX):\n",
    "    print 'Before'\n",
    "    print 'NaNs =' ,np.count_nonzero(np.isnan(theX))\n",
    "    print 'not NaNs =' ,np.count_nonzero(~np.isnan(theX))\n",
    "    rez= theX.fillna(0)\n",
    "    print 'after'\n",
    "    print 'NaNs =' ,np.count_nonzero(np.isnan(rez))\n",
    "    print 'not NaNs =' ,np.count_nonzero(~np.isnan(rez))\n",
    "    return rez\n",
    "\n",
    "'''\n",
    "Среди признаков в выборке есть категориальные, которые мы использовали как числовые, что вряд ли является хорошей \n",
    "идеей. Категориальных признаков в этой задаче одиннадцать: lobby_type и r1_hero, r2_hero, ..., r5_hero, d1_hero, \n",
    "d2_hero, ..., d5_hero. Уберите их из выборки, и проведите кросс-валидацию для логистической регрессии на новой выборке \n",
    "с подбором лучшего параметра регуляризации. Изменилось ли качество? Чем вы можете это объяснить?\n",
    "'''\n",
    "\n",
    "# replace heroes with the bag of the words\n",
    "def words_bag_of_heroes(theX):\n",
    "    # calcule heroes range\n",
    "    expected_num=113\n",
    "    unique_heroes = np.unique(theX[heroes])\n",
    "    if (max(unique_heroes)>expected_num):\n",
    "        return 0 # throw exception here\n",
    "    if (min(unique_heroes)<1):\n",
    "        return 0 # throw exception here\n",
    "    X_pick = np.zeros((theX.shape[0], expected_num))\n",
    "    for i, match_id in enumerate(theX.index):\n",
    "        for p in xrange(5):\n",
    "            X_pick[i, theX.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "            X_pick[i, theX.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1\n",
    "    return X_pick\n",
    "\n",
    "\n",
    "# добавим мешок слов по комнате\n",
    "\n",
    "'''id,name\n",
    "-1,Invalid\n",
    "0,Public matchmaking\n",
    "1,Practice\n",
    "2,Tournament\n",
    "3,Tutorial\n",
    "4,Co-op with bots\n",
    "5,Team match\n",
    "6,Solo Queue\n",
    "7,Ranked\n",
    "8,Solo Mid 1vs1\n",
    "'''\n",
    "        \n",
    "#################################################################################################\n",
    "def words_bag_of_rooms(theX):\n",
    "    num_rooms = 10\n",
    "    # check rooms range\n",
    "    num_rooms_max = 8\n",
    "    num_rooms_min = -1\n",
    "    unique_lobbyes = np.unique(theX['lobby_type'])\n",
    "    if (max(unique_lobbyes)>num_rooms_max):\n",
    "        raise NameError(max(unique_lobbyes)+'too big')  \n",
    "    if (min(unique_lobbyes) < num_rooms_min):\n",
    "        raise NameError(max(unique_lobbyes)+'too small')\n",
    "    # делаем мешок слов\n",
    "    X_pick_room = np.zeros([theX.shape[0], num_rooms])   # !!! two (())\n",
    "    for i, match_id in enumerate(theX.index):\n",
    "        for p in xrange(10):\n",
    "            X_pick_room[i, theX.ix[match_id, 'lobby_type']+1] = 1    \n",
    "    return  X_pick_room       \n",
    "\n",
    "\n",
    "\n",
    "# join X_pick heroes and rooms\n",
    "def join_bags_rooms_and_heroes(parX_pick_heroes , parX_pick_rooms):\n",
    "    print parX_pick_heroes.shape, parX_pick_rooms.shape\n",
    "    rez=np.concatenate((parX_pick_heroes , parX_pick_rooms),  axis=1)\n",
    "    print rez.shape\n",
    "    return rez\n",
    "    \n",
    "    \n",
    "# add word bag to the DataFrame\n",
    "from scipy.sparse import hstack\n",
    "def add_word_bag(parDataSet, parX_pick):\n",
    "    X_joined= add_nparray_to_dataframe(parDataSet,parX_pick, 'added')\n",
    "    # X_joined = hstack([parDataSet,parX_pick]).toarray()\n",
    "    print X_joined.shape, type(X_joined)\n",
    "    return X_joined \n",
    "\n",
    "'''\n",
    "<b>Важно</b>: не забывайте, что линейные алгоритмы чувствительны к масштабу признаков! Может пригодиться sklearn.preprocessing.StandartScaler.\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# usage\n",
    "# ss = stdScaling_fit(X)\n",
    "# X_std_scal = stdScaling_transform(X ,ss)\n",
    "def stdScaling_fit(theX):\n",
    "     return StandardScaler().fit(theX)\n",
    "\n",
    "def stdScaling_transform(theX, stdScaler):\n",
    "     return stdScaler.transform(theX)\n",
    "    \n",
    "    \n",
    "# calculate C for logistic regression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "def calculate_logreg_best_param(X_par, Y_par):\n",
    "    start_time = datetime.datetime.now()\n",
    "    param_grid = {'C': [0.00000000001, 0.0001, 0.001 ,0.01, 0.1, 1, 10, 100, 1000, 100000000] }\n",
    "\n",
    "    print \"--- initial C calculation\"\n",
    "    print param_grid\n",
    "    k_fold = KFold(len(Y_par), n_folds=5, shuffle=True, random_state=555)\n",
    "    clf = LogisticRegression(penalty='l2', random_state=555, n_jobs=-1)\n",
    "    gs_cv = GridSearchCV(clf, param_grid, scoring='roc_auc', cv=k_fold, n_jobs=-1)\n",
    "    gs_cv.fit(X_par, Y_par)\n",
    "    print \"best param = \", gs_cv.best_params_, \"best score = \", gs_cv.best_score_\n",
    "    for score in gs_cv.grid_scores_:\n",
    "        print score\n",
    "    # fine tuning for \n",
    "    fine_grid = {'C': np.add(gs_cv.best_params_.get('C'), np.multiply(np.arange(-5, 6), gs_cv.best_params_.get('C')/10))}\n",
    "    print \"--- fine C tuning\"\n",
    "    print fine_grid\n",
    "    gs_cv = GridSearchCV(clf, fine_grid, scoring='roc_auc', cv=k_fold)\n",
    "    gs_cv.fit(X_par, Y_par)\n",
    "    print \"best param = \", gs_cv.best_params_, \"best score = \", gs_cv.best_score_\n",
    "    for score in gs_cv.grid_scores_:\n",
    "        print score\n",
    "    print 'Average time for single regression fit:', (datetime.datetime.now() - start_time)/(len(param_grid.get('C')) +len(fine_grid.get('C')))\n",
    "    return gs_cv.best_params_\n",
    "\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "import datetime\n",
    "\n",
    "# cross validation of GradientBoostingClassifier \n",
    "def calculate_scores_for_estimators(n_estimators, X, y, max_depth=3):\n",
    "    print \"calculate_scores_for_estimators: start \", printtime()\n",
    "    start_time = datetime.datetime.now()\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "        random_state=555, max_depth=max_depth\n",
    "    )\n",
    "    k_fold = KFold(len(y), n_folds=5, random_state=555, shuffle=True)\n",
    "    local_scores = cross_val_score(clf, X, y, cv=k_fold, scoring='roc_auc', n_jobs=-1)\n",
    "    print 'score_mean=',local_scores.mean(), 'scores_std=',local_scores.std(), \\\n",
    "        'n_estimators =', n_estimators, 'max_depth=', max_depth, 'Time:', datetime.datetime.now() - start_time\n",
    "    print \"calculate_scores_for_estimators: finish \", printtime()\n",
    "    return local_scores.mean(), local_scores.std(), n_estimators, max_depth\n",
    "\n",
    "def crossvalidate_estimators(arr_n_estimators, X, y, max_depth=3):\n",
    "    print \"crossvalidate_estimators: start \", printtime()\n",
    "    rez_scores_local = list()\n",
    "    for n_estimators in arr_n_estimators:    \n",
    "        rez_scores_local.append(calculate_scores_for_estimators(n_estimators, X , y , max_depth=max_depth))    \n",
    "    print \"crossvalidate_estimators: finish \", printtime()    \n",
    "    return rez_scores_local\n",
    "\n",
    "def get_best_GradientBoostingClassifier(rez_scores_local):\n",
    "    # get best classifier\n",
    "    import sys\n",
    "    print type(rez_scores_local)\n",
    "    type(rez_scores_local[0])\n",
    "    max_score=-sys.maxint - 1\n",
    "    max_index=\"a\"\n",
    "    for index in range(len(rez_scores_local)):\n",
    "        if (rez_scores_local[index][0]>max_score):\n",
    "            max_index=index\n",
    "            max_score=rez_scores_local[index][0]\n",
    "            \n",
    "        print rez_scores_local[index][0]\n",
    "    best_n_estimators=rez_scores_local[max_index][2]\n",
    "    best_max_depth=rez_scores_local[max_index][3]\n",
    "    \n",
    "    print 'max_score =', max_score, 'max_index=', max_index, \\\n",
    "        'best_n_estimators=', best_n_estimators, 'best_max_depth=', best_max_depth\n",
    "    \n",
    "    clf = GradientBoostingClassifier(n_estimators=best_n_estimators,\n",
    "        random_state=555, max_depth=best_max_depth\n",
    "    )\n",
    "    return clf\n",
    "\n",
    "# calculate predicted probabilities and store to file\n",
    "#fileName = './Dota2_01/dota2-kaggle-01.csv'\n",
    "def store_predicted_probability(clf, parX, parX_match_id, fileName):\n",
    "    rez=clf.predict_proba(parX)[:, 1]\n",
    "    print 'store_predicted_probability FILE=', fileName\n",
    "    print \"Минимум предсказанных вероятностей =\",min(rez)\n",
    "    print \"Максимум предсказанных вероятностей =\", max(rez)\n",
    "    check = {\n",
    "        'match_id': parX_match_id.index, \n",
    "        'radiant_win': rez\n",
    "    }\n",
    "\n",
    "    check_dframe = pandas.DataFrame.from_dict(check)\n",
    "    check_dframe.set_index('match_id')\n",
    "\n",
    "    check_dframe.to_csv(fileName, index=False, columns=['match_id', 'radiant_win'])\n",
    "    return check_dframe    \n",
    "\n",
    "\n",
    "# prepare data for logistic regression\n",
    "import pandas\n",
    "def prepare_for_log_regr(fileName, isTest):\n",
    "    # Считайте таблицу с признаками из файла features.csv с помощью кода, приведенного выше. \n",
    "\n",
    "    features = pandas.read_csv(fileName, index_col='match_id')\n",
    "    if (number_rows_to_use>0):\n",
    "        print \"future trancated !!!!!!!!!!  to rows \", number_rows_to_use\n",
    "        # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "        features = features.sample(n=number_rows_to_use, random_state=555)\n",
    "\n",
    "    features = modify_dayaframe_01(features)\n",
    "    features = modify_dayaframe_03(features)\n",
    "    features = modify_dayaframe_02(features)\n",
    "    \n",
    "    \n",
    "    # print features.head()\n",
    "    print features.info()\n",
    "    \n",
    "    if (isTest):        \n",
    "        local_match_info = features\n",
    "    else:    \n",
    "        local_match_info, local_y = remove_result_columns(features)\n",
    "   \n",
    "    # calculate_importance(local_match_info, local_y)\n",
    "    \n",
    "    X_removed_heroes_rooms = remove_initial_heroes_and_lobbies(local_match_info)\n",
    "    X = replace_skpped_values_with_0(X_removed_heroes_rooms)\n",
    "\n",
    "    xxx_heroes= words_bag_of_heroes(local_match_info)\n",
    "    print type(xxx_heroes)\n",
    "    print xxx_heroes.shape\n",
    "\n",
    "    xxx_rooms= words_bag_of_rooms(local_match_info)\n",
    "    print type(xxx_rooms)\n",
    "    print xxx_rooms.shape\n",
    "\n",
    "    xxx_rooms_and_heroes=join_bags_rooms_and_heroes(xxx_heroes , xxx_rooms)\n",
    "\n",
    "    X_bagged = add_word_bag(X,xxx_rooms_and_heroes)\n",
    "    if (isTest):        \n",
    "        return X_bagged, local_match_info\n",
    "    else:    \n",
    "        return X_bagged, local_y, local_match_info\n",
    "\n",
    "    \n",
    "# prepare data for gradient boosting\n",
    "import pandas\n",
    "def prepare_for_gradient_boosting(fileName, isTest):\n",
    "    # Считайте таблицу с признаками из файла features.csv с помощью кода, приведенного выше. \n",
    "\n",
    "    features = pandas.read_csv(fileName, index_col='match_id')\n",
    "    if (number_rows_to_use>0):\n",
    "        print \"future trancated !!!!!!!!!!  to rows \", number_rows_to_use\n",
    "        # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\n",
    "        features = features.sample(n=number_rows_to_use, random_state=555)\n",
    "\n",
    "    features = modify_dayaframe_01(features)\n",
    "    features = modify_dayaframe_03(features)\n",
    "    #features = modify_dayaframe_02(features)\n",
    "    \n",
    "    \n",
    "    # print features.head()\n",
    "    print features.info()\n",
    "    \n",
    "    if (isTest):        \n",
    "        local_match_info = features\n",
    "    else:    \n",
    "        local_match_info, local_y = remove_result_columns(features)\n",
    "   \n",
    "    # calculate_importance(local_match_info, local_y)\n",
    "    X = replace_skpped_values_with_0(local_match_info)\n",
    "    '''\n",
    "    X_removed_heroes_rooms = remove_initial_heroes_and_lobbies(local_match_info)\n",
    "    X = replace_skpped_values_with_0(X_removed_heroes_rooms)\n",
    "\n",
    "    xxx_heroes= words_bag_of_heroes(local_match_info)\n",
    "    print type(xxx_heroes)\n",
    "    print xxx_heroes.shape\n",
    "\n",
    "    xxx_rooms= words_bag_of_rooms(local_match_info)\n",
    "    print type(xxx_rooms)\n",
    "    print xxx_rooms.shape\n",
    "\n",
    "    xxx_rooms_and_heroes=join_bags_rooms_and_heroes(xxx_heroes , xxx_rooms)\n",
    "\n",
    "    X_bagged = add_word_bag(X,xxx_rooms_and_heroes)\n",
    "    '''    \n",
    "    if (isTest):        \n",
    "        return X, local_match_info\n",
    "    else:    \n",
    "        return X, local_y, local_match_info\n",
    "    \n",
    "# sample code to append column to pandas.core.frame.DataFrame\n",
    "def append_column_to_DataFrame():\n",
    "    print x_bust_dataframe.shape\n",
    "    x_bust_dataframe['new_field'] = x_bust_dataframe['r1_lh']**2\n",
    "\n",
    "    print x_bust_dataframe['new_field'], x_bust_dataframe['r1_lh']\n",
    "    print x_bust_dataframe.shape\n",
    "\n",
    "    x_bust_dataframe['new_field_1'] = x_bust_dataframe.apply(lambda row: min([row['r1_lh'], row['r2_lh']])-row['r3_lh'], axis=1)\n",
    "    print x_bust_dataframe.shape\n",
    "\n",
    "# sample code to append column to numpy.ndarray\n",
    "def append_column_to_ndarray():\n",
    "    print x_bust.shape\n",
    "    print x_bust[:,:1].shape\n",
    "    # print x_bust[:,:1]\n",
    "    new_x=np.append(x_bust, x_bust[:,:1]**2, 1)\n",
    "    new_x.shape\n",
    "\n",
    "'''\n",
    "# Best usage is - add new field and remove 2 sources\n",
    "# считает первую кровь со знаком в одном поле\n",
    "# first_blood_time: игровое время первой крови\n",
    "# first_blood_team: команда, совершившая первую кровь (0 — Radiant, 1 — Dire)\n",
    "first_blood_player1: игрок, причастный к событию\n",
    "first_blood_player2: второй игрок, причастный к событию\n",
    "=== for log_regression: 5000\n",
    "0.735882809137   – удалил все\n",
    "0.736234081207 - удалил только 2 расчетных поля\n",
    "# === for log_regression: 1000\n",
    "# 0.716864897388- add 'first_blood_time_team', remove 'first_blood_team','first_blood_time', 'first_blood_player1','first_blood_player2'  \n",
    "# 0.717485181078- add 'first_blood_time_team', remove 'first_blood_team','first_blood_time'  \n",
    "# 0.716784645812 add 'first_blood_time_team'.\n",
    "# 0.716864897388 - original   \n",
    "# === for Gradient Boosting: \n",
    "# with this transformation \n",
    "# max_score = 0.697781663605 max_index= 0 best_n_estimators= 200 best_max_depth= 6\n",
    "# original \n",
    "# max_score = 0.698382534947 max_index= 0 best_n_estimators= 200 best_max_depth= 6\n",
    "'''\n",
    "def modify_dayaframe_01(parDataFrame):\n",
    "    print 'modify_dayaframe_01 (Before). parDataFrame', parDataFrame.shape\n",
    "    parDataFrame['first_blood_time_team'] = \\\n",
    "        parDataFrame.apply(lambda row: row['first_blood_time']*(row['first_blood_team']*2-1) , axis=1)\n",
    "    # print 'modify_dayaframe_01 (Middle). parDataFrame', parDataFrame.shape    \n",
    "    # rezDataFrame=parDataFrame\n",
    "    #rezDataFrame=parDataFrame\n",
    "    rezDataFrame=parDataFrame.drop( ['first_blood_team','first_blood_time'], axis=1)\n",
    "    # rezDataFrame=rezDataFrame.drop( ['first_blood_player1','first_blood_player2'], axis=1)\n",
    "    print 'modify_dayaframe_01 (After). parDataFrame', rezDataFrame.shape\n",
    "    return rezDataFrame\n",
    "\n",
    "# add pow(x,) fields\n",
    "# === for log_regression (1000 records)\n",
    "# 0.729154975126 - original\n",
    "# 0.736179073034 ** 2 \n",
    "# 0.743773109605 ** 3\n",
    "# 0.743972802269 ** 4\n",
    "# === for log_regression:\n",
    "# 0.714259140933add math.sqrt and remove original gold - make it even worse\n",
    "# 0.721677111535 add **2 and remove original gold - make it even worse\n",
    "\n",
    "# 0.729154975126 with function - added **2 - worse than original\n",
    "# 0.716864897388 - original   (Better !!!!!!!!!!!)\n",
    "# === for Gradient Boosting: \n",
    "# with this **2 and removed original\n",
    "# score_mean= 0.699060828574 scores_std= 0.0242692498252 n_estimators = 200 max_depth= 6\n",
    "# with  **2 - better a little\n",
    "#max_score = 0.699533440786 max_index= 0 best_n_estimators= 200 best_max_depth= 6 !!!!!!!!!!!!\n",
    "# original \n",
    "# max_score = 0.698382534947 max_index= 0 best_n_estimators= 200 best_max_depth= 6\n",
    "\n",
    "import math\n",
    "def modify_dayaframe_02(parDataFrame):\n",
    "    print 'modify_dayaframe_02 (Before). parDataFrame', parDataFrame.shape\n",
    "    power_value=power_value_config\n",
    "    toPower=['r1_gold','r2_gold','r3_gold','r4_gold','r5_gold','r_gold', \\\n",
    "             'd1_gold','d2_gold','d3_gold','d4_gold','d5_gold','d_gold',\\\n",
    "            \n",
    "             'r1_level','r2_level','r3_level','r4_level','r5_level','r_level', \\\n",
    "             'd1_level','d2_level','d3_level','d4_level','d5_level','d_level',\\\n",
    "            \n",
    "             'r1_xp','r2_xp','r3_xp','r4_xp','r5_xp', 'r_xp', \\\n",
    "             'd1_xp','d2_xp','d3_xp','d4_xp','d5_xp', 'd_xp', \\\n",
    "            \n",
    "             'r1_lh','r2_lh','r3_lh','r4_lh','r5_lh', 'r_lh', \\\n",
    "             'd1_lh','d2_lh','d3_lh','d4_lh','d5_lh','d_lh', \\\n",
    "            \n",
    "             'r1_kills','r2_kills','r3_kills','r4_kills','r5_kills', 'r_kills', \\\n",
    "             'd1_kills','d2_kills','d3_kills','d4_kills','d5_kills', 'd_kills',\\\n",
    "            \n",
    "             'r1_items','r2_items','r3_items','r4_items','r5_items','r_items', \\\n",
    "             'd1_items','d2_items','d3_items','d4_items','d5_items','d_items' \\\n",
    "            ]\n",
    "\n",
    "    for index in range(len(toPower)):\n",
    "        pow_field(toPower[index], power_value,parDataFrame)\n",
    "        #parDataFrame[powered[index]] = \\\n",
    "        #    parDataFrame.apply(lambda row: math.sqrt(row[toPower[index]]), axis=1)\n",
    "        #parDataFrame[powered[index]] = \\\n",
    "        #    parDataFrame.apply(lambda row: row[toPower[index]]**2, axis=1)\n",
    "    rezDataFrame=parDataFrame\n",
    "    # rezDataFrame=parDataFrame.drop( toPower, axis=1)\n",
    "    print 'modify_dayaframe_02 (After). rezDataFrame', rezDataFrame.shape\n",
    "    return rezDataFrame\n",
    "\n",
    "def pow_field(fld_name, mx_pwr,parDataFrame):\n",
    "    for p in xrange(mx_pwr-1):\n",
    "        parDataFrame[fld_name+'pwr'+str(p+2)] = \\\n",
    "            parDataFrame.apply(lambda row: math.pow(row[fld_name], p+2), axis=1)\n",
    "\n",
    "# Best usage is - add new field and not remove sources\n",
    "# add summ of gold R and D fields\n",
    "# === for log_regression: Gold only\n",
    "# 0.703771390136  remove original gold - \n",
    "# 0.723777307057 add summ for gold R and D and remove original gold \n",
    "# 0.729734140515 add summ for gold only R and D  \n",
    "# === for log_regression: gold..items only\n",
    "# 0.742759042342 add summ for gold..items R and D  \n",
    "# 0.716864897388 - original   (Better !!!!!!!!!!!)\n",
    "# === for Gradient Boosting: \n",
    "import math\n",
    "def modify_dayaframe_03(parDataFrame):\n",
    "    print 'modify_dayaframe_03 (Before). parDataFrame', parDataFrame.shape\n",
    "    add_sum_field('r', '_gold', parDataFrame)\n",
    "    add_sum_field('d', '_gold', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_level', parDataFrame)\n",
    "    add_sum_field('d', '_level', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_xp', parDataFrame)\n",
    "    add_sum_field('d', '_xp', parDataFrame)\n",
    "\n",
    "\n",
    "    add_sum_field('r', '_lh', parDataFrame)\n",
    "    add_sum_field('d', '_lh', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_kills', parDataFrame)\n",
    "    add_sum_field('d', '_kills', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_deaths', parDataFrame)\n",
    "    add_sum_field('d', '_deaths', parDataFrame)\n",
    "\n",
    "    add_sum_field('r', '_items', parDataFrame)\n",
    "    add_sum_field('d', '_items', parDataFrame)\n",
    "\n",
    "\n",
    "    '''\n",
    "    toPower=['r1_gold','r2_gold','r3_gold','r4_gold','r5_gold', \\\n",
    "             'd1_gold','d2_gold','d3_gold','d4_gold','d5_gold']\n",
    "    powered=['r1_gold2','r2_gold2','r3_gold2','r4_gold2','r5_gold2', \\\n",
    "             'd1_gold2','d2_gold2','d3_gold2','d4_gold2','d5_gold2']\n",
    "    parDataFrame['r_gold'] = parDataFrame['r1_gold']+ parDataFrame['r2_gold']+ parDataFrame['r3_gold'] + \\\n",
    "        parDataFrame['r4_gold']+parDataFrame['r5_gold']\n",
    "    parDataFrame['d_gold'] = parDataFrame['d1_gold']+ parDataFrame['d2_gold']+ parDataFrame['d3_gold'] + \\\n",
    "        parDataFrame['d4_gold']+parDataFrame['d5_gold']\n",
    "    '''\n",
    "    rezDataFrame=parDataFrame\n",
    "    # rezDataFrame=parDataFrame.drop( toPower, axis=1)\n",
    "    print 'modify_dayaframe_03 (After). rezDataFrame', rezDataFrame.shape\n",
    "    return rezDataFrame\n",
    "\n",
    "# add_sum_field('r', '_gold', parDataFrame)\n",
    "def add_sum_field(r_d, fd_name, parDataFrame):\n",
    "    parDataFrame[r_d+fd_name] = parDataFrame[r_d+'1'+fd_name]+ parDataFrame[r_d+'2'+fd_name]+ parDataFrame[r_d+'3'+fd_name] + \\\n",
    "        parDataFrame[r_d+'4'+fd_name]+parDataFrame[r_d+'5'+fd_name]\n",
    "\n",
    "'''\n",
    "        'r%d_hero' % (p+1)]\n",
    "            X_pick[i, theX.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "            X_pick[i, theX.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1\n",
    "'''    \n",
    "# \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def calculate_importance(theX, theY):\n",
    "    clf = DecisionTreeClassifier(random_state=241)\n",
    "    clf.fit(theX, theY)\n",
    "    importances = clf.feature_importances_\n",
    "    print importances\n",
    "    print importances.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some draft code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# prepare data for gradient boosting    ?????????????????????\n",
    "import pandas\n",
    "def prepare_for_boosting(fileName, isTest):\n",
    "    # Считайте таблицу с признаками из файла features.csv с помощью кода, приведенного выше. \n",
    "\n",
    "    features = pandas.read_csv(fileName, index_col='match_id')\n",
    "    print features.head()\n",
    "    print features.info()\n",
    "    \n",
    "    if (isTest):        \n",
    "        match_info = features\n",
    "    else:    \n",
    "        match_info, y = remove_result_columns(features)\n",
    "        \n",
    "    X_removed_heroes_rooms = remove_initial_heroes_and_lobbies(match_info)\n",
    "    X = replace_skpped_values_with_0(X_removed_heroes_rooms)\n",
    "\n",
    "    xxx_heroes= words_bag_of_heroes(match_info)\n",
    "    print type(xxx_heroes)\n",
    "    print xxx_heroes.shape\n",
    "\n",
    "    xxx_rooms= words_bag_of_rooms(match_info)\n",
    "    print type(xxx_rooms)\n",
    "    print xxx_rooms.shape\n",
    "\n",
    "    xxx_rooms_and_heroes=join_bags_rooms_and_heroes(xxx_heroes , xxx_rooms)\n",
    "\n",
    "    X_bagged = add_word_bag(X,xxx_rooms_and_heroes)\n",
    "    if (isTest):        \n",
    "        return X_bagged, X\n",
    "    else:    \n",
    "        return X_bagged, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Skip it ++++++++++++++++++++++\n",
    "# Считаем для 30 - max_depth=6\n",
    "rez_scores_local = list()\n",
    "for n_estimators in [1,2]:    \n",
    "    rez_scores_local.append((n_estimators, calculate_scores_for_estimators(n_estimators, x_bust , y_bust, max_depth=1)))    \n",
    "rez_scores_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# check if max score is good\n",
    "import sys\n",
    "print type(rez_scores_local)\n",
    "type(rez_scores_local[0])\n",
    "max_score=-sys.maxint - 1\n",
    "max_index=\"a\"\n",
    "for index in range(len(rez_scores_local)):\n",
    "    if (rez_scores_local[index][1][0]>max_score):\n",
    "        max_index=index\n",
    "    print rez_scores_local[index][1][0]\n",
    "print max_score, max_index\n",
    "\n",
    "#   print 'Current fruit :', fruits[index]\n",
    "# rez_scores_local[:]\n",
    "#[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "x_t, y_t, x_data_frame  = prepare_for_gradient_boosting(feature_train_file_config, False)\n",
    "\n",
    "additional_rez_file='./Dota2_03/ExportedRez/logreg_train_01_02_03_pow4.csv'\n",
    "x_data_frame =add_calculated_prob(additional_rez_file, x_data_frame )\n",
    "\n",
    "x_data_frame = x_data_frame[['radiant_win', 'r_gold']]\n",
    "\n",
    "xxxx_dataframe = replace_skpped_values_with_0(x_data_frame)\n",
    "clf = DecisionTreeClassifier(random_state=241)\n",
    "clf.fit(xxxx_dataframe , y_t)\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "for index in range(len(importances)):\n",
    "    print pearsonr(xxxx_dataframe[xxxx_dataframe.columns[index]] , y_t), xxxx_dataframe.columns[index] \n",
    "\n",
    "\n",
    "for index in range(len(importances)):\n",
    "    print importances[index] , xxxx_dataframe.columns[index] \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression - train - all - with usage results of gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score \n",
    "def len_reg_train_test_plus_gradient_boosting(out_file_suffix, additional_train_file, additional_test_file ):\n",
    "    \n",
    "    feature_file=feature_train_file_config\n",
    "    test_file=feature_test_file_config\n",
    "    out_file_train='logreg_train_01_02_03_pow2_'+out_file_suffix+'.csv'\n",
    "    out_file_test='logreg_test_01_02_03_pow2_'+out_file_suffix+'.csv'\n",
    "\n",
    "    print '############## logistic regression', out_file_suffix\n",
    "    print 'added file TRAIN =',additional_train_file\n",
    "    print 'added file TEST =',additional_test_file\n",
    "\n",
    "\n",
    "    # training\n",
    "    print '---Start train' , printtime() \n",
    "\n",
    "    x_train, y_train, x_train_data_frame  = prepare_for_log_regr(feature_file, False)\n",
    "    print type(x_train)\n",
    "\n",
    "    # add addition filed with linear regression result\n",
    "    #add_random_column_to_datafram(x_train, 'radiant_win')   \n",
    "    \n",
    "    x_train=add_calculated_prob(additional_train_file, x_train )\n",
    "    print 'roc_auc_score=',roc_auc_score(y_train['radiant_win'], x_train['radiant_win'] )\n",
    "    print 'Done0'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ss = stdScaling_fit(x_train)\n",
    "    print 'Done1'\n",
    "\n",
    "\n",
    "    X_train_std_scal = stdScaling_transform( x_train ,ss)\n",
    "    print 'Done2'\n",
    "\n",
    "    log_reg_best_param = calculate_logreg_best_param(X_train_std_scal, y_train)\n",
    "    print 'Done3'\n",
    "\n",
    "    clf = LogisticRegression(penalty='l2', random_state=555, C=log_reg_best_param['C']) \n",
    "    print 'Done4'\n",
    "\n",
    "    \n",
    "\n",
    "    clf.fit(X_train_std_scal, y_train)\n",
    "    print 'Done5'\n",
    "    rez_dataframe = store_predicted_probability(clf, X_train_std_scal, x_train_data_frame, out_file_train)\n",
    "    print 'Done6' , printtime() \n",
    "    \n",
    "    # test\n",
    "    print '--- Start test ', printtime() \n",
    "    X_test, X_test_data_frame = prepare_for_log_regr(test_file, True)\n",
    "    # add addition filed with linear regression result\n",
    "    X_test=add_calculated_prob(additional_test_file, X_test )\n",
    "    print '=++++++++= X_test.shape ', X_test.shape\n",
    "    X_test_std_scal = stdScaling_transform( X_test ,ss)\n",
    "    rez_dataframe = store_predicted_probability(clf, X_test_std_scal, X_test_data_frame, out_file_test)\n",
    "\n",
    "\n",
    "    print 'Done'    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print 'Start _ all', printtime()  \n",
    "'''\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 6\n",
    "0.722038617167\n",
    "\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 5\n",
    "0.723343393486\n",
    "\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 4\n",
    "0.722611854863\n",
    "\n",
    "\n",
    "gradient boosting: trainig. Estimators=  10 max_depth= 2\n",
    "0.703845172719\n",
    "'''\n",
    "grad_boosting_folder = 'Dota2_03/ExportedRez_grad_boost/'\n",
    "\n",
    "additional_train_file=grad_boosting_folder+'grad_boost_train_120_6.csv'\n",
    "additional_test_file=grad_boosting_folder+'grad_boost_test_120_6.csv'\n",
    "len_reg_train_test_plus_gradient_boosting('gb_120_6', additional_train_file, additional_test_file)\n",
    "\n",
    "\n",
    "additional_train_file=grad_boosting_folder+'grad_boost_train_120_5.csv'\n",
    "additional_test_file=grad_boosting_folder+'grad_boost_test_120_5.csv'\n",
    "len_reg_train_test_plus_gradient_boosting('gb_120_5', additional_train_file, additional_test_file)\n",
    "\n",
    "additional_train_file='grad_boosting_folder+'grad_boost_train_120_4.csv'\n",
    "additional_test_file=grad_boosting_folder+'grad_boost_test_120_4.csv'\n",
    "len_reg_train_test_plus_gradient_boosting('gb_120_4', additional_train_file, additional_test_file)\n",
    "\n",
    "\n",
    "additional_train_file=grad_boosting_folder+'grad_boost_train_10_2.csv'\n",
    "additional_test_file=grad_boosting_folder+'grad_boost_test_10_2.csv'\n",
    "len_reg_train_test_plus_gradient_boosting('gb_10_2', additional_train_file, additional_test_file)\n",
    "\n",
    "\n",
    "print 'Done _ all', printtime()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_train_data_frame[['first_blood_time_team','first_blood_time','first_blood_team']]\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print 'Start' , printtime() \n",
    "x_train, y_train, x_train_data_frame,  = prepare_for_log_regr(feature_train_file_config, False)\n",
    "ss = stdScaling_fit(x_train)\n",
    "print 'Done1'\n",
    "X_train_std_scal = stdScaling_transform( x_train ,ss)\n",
    "print 'Done2'\n",
    "\n",
    "log_reg_best_param = calculate_logreg_best_param(X_train_std_scal, y_train)\n",
    "print 'Done3'\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', random_state=555, C=log_reg_best_param['C']) \n",
    "print 'Done4'\n",
    "\n",
    "clf.fit(X_train_std_scal, y_train)\n",
    "\n",
    "print 'Done5'\n",
    "rez_dataframe = store_predicted_probability(clf, X_train_std_scal, x_train_data_frame, './Dota2_03/logreg_train_01_02_03_pow2.csv')\n",
    "print 'Done6' , printtime() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## x_train, y_train, x_train_data_frame  = prepare_for_log_regr(feature_train_file_config, False)\n",
    "ss = stdScaling_fit(x_train)\n",
    "print 'Done1'\n",
    "X_train_std_scal = stdScaling_transform( x_train ,ss)\n",
    "print 'Done2'\n",
    "\n",
    "log_reg_best_param = calculate_logreg_best_param(X_train_std_scal, y_train)\n",
    "print 'Done3'\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', random_state=555, C=log_reg_best_param['C']) # !! replace with log_reg_best_param\n",
    "print 'Done4'\n",
    "\n",
    "clf.fit(X_train_std_scal, y_train)\n",
    "\n",
    "print 'Done5'\n",
    "rez_dataframe = store_predicted_probability(clf, X_train_std_scal, x_train_data_frame, './Dota2_02/logreg_train.csv')\n",
    "print 'Done6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "log_reg_best_param = calculate_logreg_best_param(X_std_scal)\n",
    "log_reg_best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_test, X_test_data_frame = prepare_for_log_regr(feature_test_file_config, True)\n",
    "X_test_std_scal = stdScaling_transform( X_test ,ss)\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rez_dataframe = store_predicted_probability(clf, X_test_std_scal, X_test_data_frame, './Dota2_02/logreg_test_01_02_03.csv')\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_train_test(par_estimators, par_max_depth):\n",
    "    out_folder_cnf = './'\n",
    "    feature_file=feature_train_file_config\n",
    "    test_file=feature_test_file_config\n",
    "    # training\n",
    "    print '############## gradient boosting: trainig. Estimators= ' , par_estimators[0], 'max_depth=',  par_max_depth \n",
    "    out_train_file=out_folder_cnf+'grad_boost_train_'+str(par_estimators[0])+'_'+str(par_max_depth)+'.csv'\n",
    "    out_test_file=out_folder_cnf+'grad_boost_test_'+str(par_estimators[0])+'_'+str(par_max_depth)+'.csv'    \n",
    "    print '---Start train' , printtime() \n",
    "    x_train_bust, y_train_bust, x_train_data_frame_bust  = prepare_for_gradient_boosting(feature_file, False)\n",
    "    print 'Done1'\n",
    "\n",
    "    rez_scores = crossvalidate_estimators(par_estimators, x_train_bust, y_train_bust, max_depth=par_max_depth)\n",
    "    print 'Done2', printtime()   \n",
    "\n",
    "    clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "    clf.fit(x_train_bust, y_train_bust)\n",
    "    print 'Done3', printtime()   \n",
    "\n",
    "    rez_dataframe = store_predicted_probability(clf, x_train_bust, x_train_data_frame_bust, out_train_file)\n",
    "    print 'Done4', printtime()\n",
    "    # test\n",
    "    print '--- Start test ', printtime()  \n",
    "    \n",
    "    x_test, X_test_data_frame = prepare_for_gradient_boosting(test_file, True)\n",
    "\n",
    "    rez_dataframe = store_predicted_probability(clf, x_test, X_test_data_frame, out_test_file)\n",
    "\n",
    "    print 'Done', printtime() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print 'Start _ all', printtime()  \n",
    "gradient_boosting_train_test([120], 6)\n",
    "'''\n",
    "gradient_boosting_train_test([10], 3)\n",
    "gradient_boosting_train_test([10], 4)\n",
    "gradient_boosting_train_test([20], 2)\n",
    "gradient_boosting_train_test([30], 2)\n",
    "gradient_boosting_train_test([30], 4)\n",
    "gradient_boosting_train_test([30], 5)\n",
    "'''\n",
    "print 'Done _ all', printtime() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг - с добалением результатов линейной регресси"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_train_test_plus_len_reg(par_estimators, par_max_depth, additional_train_file, additional_test_file ):\n",
    "    out_folder_cnf = './'\n",
    "    feature_file=feature_train_file_config\n",
    "    test_file=feature_test_file_config\n",
    "\n",
    "    out_train_file=out_folder_cnf+'grad_boost_train_'+str(par_estimators[0])+'_'+str(par_max_depth)+'_plus_logreg.csv'\n",
    "    out_test_file=out_folder_cnf+'grad_boost_test_'+str(par_estimators[0])+'_'+str(par_max_depth)+'_plus_logreg.csv'    \n",
    "\n",
    "    print '############## gradient boosting: trainig. Estimators= ' , par_estimators[0], 'max_depth=',  par_max_depth \n",
    "    print 'added file TRAIN =',additional_train_file\n",
    "    print 'added file TEST =',additional_test_file\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # training\n",
    "    print '---Start train' , printtime() \n",
    "    x_train_bust, y_train_bust, x_train_data_frame_bust  = prepare_for_gradient_boosting(feature_file, False)\n",
    "    print 'Done1'\n",
    "    \n",
    "    print type(x_train_bust)\n",
    "\n",
    "    # add addition filed with linear regression result\n",
    "    x_train_bust=add_calculated_prob(additional_train_file, x_train_bust )\n",
    "    \n",
    "    rez_scores = crossvalidate_estimators(par_estimators, x_train_bust, y_train_bust, max_depth=par_max_depth)\n",
    "\n",
    "    print 'Done2', printtime()   \n",
    "\n",
    "    clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "    clf.fit(x_train_bust, y_train_bust)\n",
    "    print 'Done3', printtime()   \n",
    "    rez_dataframe = store_predicted_probability(clf, x_train_bust, x_train_data_frame_bust, out_train_file)\n",
    "    print 'Done4', printtime()\n",
    "    \n",
    "    # test\n",
    "    print '--- Start test ', printtime()  \n",
    "    \n",
    "    x_test, X_test_data_frame = prepare_for_gradient_boosting(test_file, True)\n",
    "    # add addition filed with linear regression result\n",
    "    x_test=add_calculated_prob(additional_test_file, x_test )\n",
    "\n",
    "    rez_dataframe = store_predicted_probability(clf, x_test, X_test_data_frame, out_test_file)\n",
    "\n",
    "    print 'Done', printtime()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start _ all 2016-05-04 00:54:01.186000\n",
      "############## gradient boosting: trainig. Estimators=  120 max_depth= 6\n",
      "added file TRAIN = C:/My/Dota2/03/ExportedRez_log_reg/logreg_train_01_02_03_pow2.csv\n",
      "added file TEST = C:/My/Dota2/03/ExportedRez_log_reg/logreg_test_01_02_03_pow2.csv\n",
      "---Start train 2016-05-04 00:54:01.188000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 121 entries, start_time to d_items\n",
      "dtypes: float64(11), int64(110)\n",
      "memory usage: 90.5 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 121)\n",
      "remove_result_columns (After): shape (97230, 115)\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 11007916\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 11181450\n",
      "Done1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "crossvalidate_estimators: start  2016-05-04 00:54:10.663000\n",
      "calculate_scores_for_estimators: start  2016-05-04 00:54:10.663000\n",
      "score_mean= 0.753566204425 scores_std= 0.00391650456146 n_estimators = 120 max_depth= 6 Time: 0:09:30.104000\n",
      "calculate_scores_for_estimators: finish  2016-05-04 01:03:40.767000\n",
      "crossvalidate_estimators: finish  2016-05-04 01:03:40.767000\n",
      "Done2 2016-05-04 01:03:40.767000\n",
      "<type 'list'>\n",
      "0.753566204425\n",
      "max_score = 0.753566204425 max_index= 0 best_n_estimators= 120 best_max_depth= 6\n",
      "Done3 2016-05-04 01:11:48.804000\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/grad_boost_train_120_6_plus_logreg.csv\n",
      "Минимум предсказанных вероятностей = 0.0171904085986\n",
      "Максимум предсказанных вероятностей = 0.987914988364\n",
      "Done4 2016-05-04 01:11:50.797000\n",
      "--- Start test  2016-05-04 01:11:50.797000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 115 entries, start_time to d_items\n",
      "dtypes: float64(11), int64(104)\n",
      "memory usage: 15.2 MB\n",
      "None\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 1944489\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 1975355\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/grad_boost_test_120_6_plus_logreg.csv\n",
      "Минимум предсказанных вероятностей = 0.0189597455463\n",
      "Максимум предсказанных вероятностей = 0.986289472481\n",
      "Done 2016-05-04 01:11:52.899000\n",
      "############## gradient boosting: trainig. Estimators=  120 max_depth= 5\n",
      "added file TRAIN = C:/My/Dota2/03/ExportedRez_log_reg/logreg_train_01_02_03_pow2.csv\n",
      "added file TEST = C:/My/Dota2/03/ExportedRez_log_reg/logreg_test_01_02_03_pow2.csv\n",
      "---Start train 2016-05-04 01:11:52.919000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 121 entries, start_time to d_items\n",
      "dtypes: float64(11), int64(110)\n",
      "memory usage: 90.5 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 121)\n",
      "remove_result_columns (After): shape (97230, 115)\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 11007916\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 11181450\n",
      "Done1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "crossvalidate_estimators: start  2016-05-04 01:12:02.300000\n",
      "calculate_scores_for_estimators: start  2016-05-04 01:12:02.300000\n",
      "score_mean= 0.75497010374 scores_std= 0.00370295927146 n_estimators = 120 max_depth= 5 Time: 0:06:35.635000\n",
      "calculate_scores_for_estimators: finish  2016-05-04 01:18:37.935000\n",
      "crossvalidate_estimators: finish  2016-05-04 01:18:37.935000\n",
      "Done2 2016-05-04 01:18:37.935000\n",
      "<type 'list'>\n",
      "0.75497010374\n",
      "max_score = 0.75497010374 max_index= 0 best_n_estimators= 120 best_max_depth= 5\n",
      "Done3 2016-05-04 01:24:06.694000\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/grad_boost_train_120_5_plus_logreg.csv\n",
      "Минимум предсказанных вероятностей = 0.0242729905399\n",
      "Максимум предсказанных вероятностей = 0.983463688507\n",
      "Done4 2016-05-04 01:24:08.668000\n",
      "--- Start test  2016-05-04 01:24:08.668000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 115 entries, start_time to d_items\n",
      "dtypes: float64(11), int64(104)\n",
      "memory usage: 15.2 MB\n",
      "None\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 1944489\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 1975355\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/grad_boost_test_120_5_plus_logreg.csv\n",
      "Минимум предсказанных вероятностей = 0.0310372962959\n",
      "Максимум предсказанных вероятностей = 0.979893663141\n",
      "Done 2016-05-04 01:24:10.865000\n",
      "############## gradient boosting: trainig. Estimators=  120 max_depth= 4\n",
      "added file TRAIN = C:/My/Dota2/03/ExportedRez_log_reg/logreg_train_01_02_03_pow2.csv\n",
      "added file TEST = C:/My/Dota2/03/ExportedRez_log_reg/logreg_test_01_02_03_pow2.csv\n",
      "---Start train 2016-05-04 01:24:10.888000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 121 entries, start_time to d_items\n",
      "dtypes: float64(11), int64(110)\n",
      "memory usage: 90.5 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 121)\n",
      "remove_result_columns (After): shape (97230, 115)\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 11007916\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 11181450\n",
      "Done1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "crossvalidate_estimators: start  2016-05-04 01:24:20.232000\n",
      "calculate_scores_for_estimators: start  2016-05-04 01:24:20.232000\n",
      "score_mean= 0.755888467732 scores_std= 0.00350303576086 n_estimators = 120 max_depth= 4 Time: 0:04:35.792000\n",
      "calculate_scores_for_estimators: finish  2016-05-04 01:28:56.024000\n",
      "crossvalidate_estimators: finish  2016-05-04 01:28:56.024000\n",
      "Done2 2016-05-04 01:28:56.024000\n",
      "<type 'list'>\n",
      "0.755888467732\n",
      "max_score = 0.755888467732 max_index= 0 best_n_estimators= 120 best_max_depth= 4\n",
      "Done3 2016-05-04 01:32:28.960000\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/grad_boost_train_120_4_plus_logreg.csv\n",
      "Минимум предсказанных вероятностей = 0.0320733403831\n",
      "Максимум предсказанных вероятностей = 0.978666419016\n",
      "Done4 2016-05-04 01:32:30.656000\n",
      "--- Start test  2016-05-04 01:32:30.656000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 115 entries, start_time to d_items\n",
      "dtypes: float64(11), int64(104)\n",
      "memory usage: 15.2 MB\n",
      "None\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 1944489\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 1975355\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/grad_boost_test_120_4_plus_logreg.csv\n",
      "Минимум предсказанных вероятностей = 0.0374057346574\n",
      "Максимум предсказанных вероятностей = 0.976392985149\n",
      "Done 2016-05-04 01:32:32.763000\n",
      "############## gradient boosting: trainig. Estimators=  10 max_depth= 2\n",
      "added file TRAIN = C:/My/Dota2/03/ExportedRez_log_reg/logreg_train_01_02_03_pow2.csv\n",
      "added file TEST = C:/My/Dota2/03/ExportedRez_log_reg/logreg_test_01_02_03_pow2.csv\n",
      "---Start train 2016-05-04 01:32:32.784000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 121 entries, start_time to d_items\n",
      "dtypes: float64(11), int64(110)\n",
      "memory usage: 90.5 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 121)\n",
      "remove_result_columns (After): shape (97230, 115)\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 11007916\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 11181450\n",
      "Done1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "crossvalidate_estimators: start  2016-05-04 01:32:42.048000\n",
      "calculate_scores_for_estimators: start  2016-05-04 01:32:42.048000\n",
      "score_mean= 0.755347929013 scores_std= 0.0037281910584 n_estimators = 10 max_depth= 2 Time: 0:00:17.219000\n",
      "calculate_scores_for_estimators: finish  2016-05-04 01:32:59.267000\n",
      "crossvalidate_estimators: finish  2016-05-04 01:32:59.267000\n",
      "Done2 2016-05-04 01:32:59.267000\n",
      "<type 'list'>\n",
      "0.755347929013\n",
      "max_score = 0.755347929013 max_index= 0 best_n_estimators= 10 best_max_depth= 2\n",
      "Done3 2016-05-04 01:33:07.655000\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/grad_boost_train_10_2_plus_logreg.csv\n",
      "Минимум предсказанных вероятностей = 0.295720819908\n",
      "Максимум предсказанных вероятностей = 0.740562977941\n",
      "Done4 2016-05-04 01:33:08.376000\n",
      "--- Start test  2016-05-04 01:33:08.376000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 115 entries, start_time to d_items\n",
      "dtypes: float64(11), int64(104)\n",
      "memory usage: 15.2 MB\n",
      "None\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 1944489\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 1975355\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/grad_boost_test_10_2_plus_logreg.csv\n",
      "Минимум предсказанных вероятностей = 0.295720819908\n",
      "Максимум предсказанных вероятностей = 0.740562977941\n",
      "Done 2016-05-04 01:33:10.283000\n",
      "Done _ all 2016-05-04 01:33:10.308000\n",
      "Start _ all 2016-05-04 01:33:10.308000\n",
      "############## logistic regression gb_120_6\n",
      "added file TRAIN = C:/My/Dota2/03/ExportedRez_grad_boost/grad_boost_train_120_6.csv\n",
      "added file TEST = C:/My/Dota2/03/ExportedRez_grad_boost/grad_boost_test_120_6.csv\n",
      "---Start train 2016-05-04 01:33:10.308000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (Before). parDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (After). rezDataFrame (97230, 193)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 193 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(110)\n",
      "memory usage: 143.9 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 193)\n",
      "remove_result_columns (After): shape (97230, 187)\n",
      "(97230, 187) <class 'pandas.core.frame.DataFrame'>\n",
      "(97230, 176) <class 'pandas.core.frame.DataFrame'>\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 16938946\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 17112480\n",
      "<type 'numpy.ndarray'>\n",
      "(97230L, 113L)\n",
      "<type 'numpy.ndarray'>\n",
      "(97230L, 10L)\n",
      "(97230L, 113L) (97230L, 10L)\n",
      "(97230L, 123L)\n",
      "added columns:  ['added0', 'added1', 'added2', 'added3', 'added4', 'added5', 'added6', 'added7', 'added8', 'added9', 'added10', 'added11', 'added12', 'added13', 'added14', 'added15', 'added16', 'added17', 'added18', 'added19', 'added20', 'added21', 'added22', 'added23', 'added24', 'added25', 'added26', 'added27', 'added28', 'added29', 'added30', 'added31', 'added32', 'added33', 'added34', 'added35', 'added36', 'added37', 'added38', 'added39', 'added40', 'added41', 'added42', 'added43', 'added44', 'added45', 'added46', 'added47', 'added48', 'added49', 'added50', 'added51', 'added52', 'added53', 'added54', 'added55', 'added56', 'added57', 'added58', 'added59', 'added60', 'added61', 'added62', 'added63', 'added64', 'added65', 'added66', 'added67', 'added68', 'added69', 'added70', 'added71', 'added72', 'added73', 'added74', 'added75', 'added76', 'added77', 'added78', 'added79', 'added80', 'added81', 'added82', 'added83', 'added84', 'added85', 'added86', 'added87', 'added88', 'added89', 'added90', 'added91', 'added92', 'added93', 'added94', 'added95', 'added96', 'added97', 'added98', 'added99', 'added100', 'added101', 'added102', 'added103', 'added104', 'added105', 'added106', 'added107', 'added108', 'added109', 'added110', 'added111', 'added112', 'added113', 'added114', 'added115', 'added116', 'added117', 'added118', 'added119', 'added120', 'added121', 'added122']\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "(97230, 299) <class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "roc_auc_score= 0.803214610299\n",
      "Done0\n",
      "Done1\n",
      "Done2\n",
      "--- initial C calculation\n",
      "{'C': [1e-11, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 100000000]}\n",
      "best param =  {'C': 1} best score =  0.863028850866\n",
      "mean: 0.54010, std: 0.00669, params: {'C': 1e-11}\n",
      "mean: 0.79224, std: 0.00360, params: {'C': 0.0001}\n",
      "mean: 0.84752, std: 0.00262, params: {'C': 0.001}\n",
      "mean: 0.86098, std: 0.00217, params: {'C': 0.01}\n",
      "mean: 0.86292, std: 0.00202, params: {'C': 0.1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86302, std: 0.00199, params: {'C': 10}\n",
      "mean: 0.86302, std: 0.00199, params: {'C': 100}\n",
      "mean: 0.86302, std: 0.00199, params: {'C': 1000}\n",
      "mean: 0.86302, std: 0.00199, params: {'C': 100000000}\n",
      "--- fine C tuning\n",
      "{'C': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "best param =  {'C': 1} best score =  0.863028850866\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "mean: 0.86303, std: 0.00199, params: {'C': 1}\n",
      "Average time for single regression fit: 0:02:36.486190\n",
      "Done3\n",
      "Done4\n",
      "Done5\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/logreg_train_01_02_03_pow2_gb_120_6.csv\n",
      "Минимум предсказанных вероятностей = 2.35297985699e-07\n",
      "Максимум предсказанных вероятностей = 0.999941592556\n",
      "Done6 2016-05-04 02:33:43.953000\n",
      "--- Start test  2016-05-04 02:33:43.953000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (Before). parDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (After). rezDataFrame (17177, 187)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 187 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(104)\n",
      "memory usage: 24.6 MB\n",
      "None\n",
      "(17177, 187) <class 'pandas.core.frame.DataFrame'>\n",
      "(17177, 176) <class 'pandas.core.frame.DataFrame'>\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 2992286\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 3023152\n",
      "<type 'numpy.ndarray'>\n",
      "(17177L, 113L)\n",
      "<type 'numpy.ndarray'>\n",
      "(17177L, 10L)\n",
      "(17177L, 113L) (17177L, 10L)\n",
      "(17177L, 123L)\n",
      "added columns:  ['added0', 'added1', 'added2', 'added3', 'added4', 'added5', 'added6', 'added7', 'added8', 'added9', 'added10', 'added11', 'added12', 'added13', 'added14', 'added15', 'added16', 'added17', 'added18', 'added19', 'added20', 'added21', 'added22', 'added23', 'added24', 'added25', 'added26', 'added27', 'added28', 'added29', 'added30', 'added31', 'added32', 'added33', 'added34', 'added35', 'added36', 'added37', 'added38', 'added39', 'added40', 'added41', 'added42', 'added43', 'added44', 'added45', 'added46', 'added47', 'added48', 'added49', 'added50', 'added51', 'added52', 'added53', 'added54', 'added55', 'added56', 'added57', 'added58', 'added59', 'added60', 'added61', 'added62', 'added63', 'added64', 'added65', 'added66', 'added67', 'added68', 'added69', 'added70', 'added71', 'added72', 'added73', 'added74', 'added75', 'added76', 'added77', 'added78', 'added79', 'added80', 'added81', 'added82', 'added83', 'added84', 'added85', 'added86', 'added87', 'added88', 'added89', 'added90', 'added91', 'added92', 'added93', 'added94', 'added95', 'added96', 'added97', 'added98', 'added99', 'added100', 'added101', 'added102', 'added103', 'added104', 'added105', 'added106', 'added107', 'added108', 'added109', 'added110', 'added111', 'added112', 'added113', 'added114', 'added115', 'added116', 'added117', 'added118', 'added119', 'added120', 'added121', 'added122']\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "(17177, 299) <class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "=++++++++= X_test.shape  (17177, 300)\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/logreg_test_01_02_03_pow2_gb_120_6.csv\n",
      "Минимум предсказанных вероятностей = 0.00229216808183\n",
      "Максимум предсказанных вероятностей = 0.99785889087\n",
      "Done\n",
      "############## logistic regression gb_120_5\n",
      "added file TRAIN = C:/My/Dota2/03/ExportedRez_grad_boost/grad_boost_train_120_5.csv\n",
      "added file TEST = C:/My/Dota2/03/ExportedRez_grad_boost/grad_boost_test_120_5.csv\n",
      "---Start train 2016-05-04 02:34:35.144000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (Before). parDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (After). rezDataFrame (97230, 193)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 193 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(110)\n",
      "memory usage: 143.9 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 193)\n",
      "remove_result_columns (After): shape (97230, 187)\n",
      "(97230, 187) <class 'pandas.core.frame.DataFrame'>\n",
      "(97230, 176) <class 'pandas.core.frame.DataFrame'>\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 16938946\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 17112480\n",
      "<type 'numpy.ndarray'>\n",
      "(97230L, 113L)\n",
      "<type 'numpy.ndarray'>\n",
      "(97230L, 10L)\n",
      "(97230L, 113L) (97230L, 10L)\n",
      "(97230L, 123L)\n",
      "added columns:  ['added0', 'added1', 'added2', 'added3', 'added4', 'added5', 'added6', 'added7', 'added8', 'added9', 'added10', 'added11', 'added12', 'added13', 'added14', 'added15', 'added16', 'added17', 'added18', 'added19', 'added20', 'added21', 'added22', 'added23', 'added24', 'added25', 'added26', 'added27', 'added28', 'added29', 'added30', 'added31', 'added32', 'added33', 'added34', 'added35', 'added36', 'added37', 'added38', 'added39', 'added40', 'added41', 'added42', 'added43', 'added44', 'added45', 'added46', 'added47', 'added48', 'added49', 'added50', 'added51', 'added52', 'added53', 'added54', 'added55', 'added56', 'added57', 'added58', 'added59', 'added60', 'added61', 'added62', 'added63', 'added64', 'added65', 'added66', 'added67', 'added68', 'added69', 'added70', 'added71', 'added72', 'added73', 'added74', 'added75', 'added76', 'added77', 'added78', 'added79', 'added80', 'added81', 'added82', 'added83', 'added84', 'added85', 'added86', 'added87', 'added88', 'added89', 'added90', 'added91', 'added92', 'added93', 'added94', 'added95', 'added96', 'added97', 'added98', 'added99', 'added100', 'added101', 'added102', 'added103', 'added104', 'added105', 'added106', 'added107', 'added108', 'added109', 'added110', 'added111', 'added112', 'added113', 'added114', 'added115', 'added116', 'added117', 'added118', 'added119', 'added120', 'added121', 'added122']\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "(97230, 299) <class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "roc_auc_score= 0.768988543401\n",
      "Done0\n",
      "Done1\n",
      "Done2\n",
      "--- initial C calculation\n",
      "{'C': [1e-11, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 100000000]}\n",
      "best param =  {'C': 1} best score =  0.811698563281\n",
      "mean: 0.53983, std: 0.00172, params: {'C': 1e-11}\n",
      "mean: 0.76655, std: 0.00367, params: {'C': 0.0001}\n",
      "mean: 0.79877, std: 0.00321, params: {'C': 0.001}\n",
      "mean: 0.81016, std: 0.00267, params: {'C': 0.01}\n",
      "mean: 0.81162, std: 0.00255, params: {'C': 0.1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 10}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 100}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1000}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 100000000}\n",
      "--- fine C tuning\n",
      "{'C': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "best param =  {'C': 1} best score =  0.811698563281\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "mean: 0.81170, std: 0.00254, params: {'C': 1}\n",
      "Average time for single regression fit: 0:02:56.104285\n",
      "Done3\n",
      "Done4\n",
      "Done5\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/logreg_train_01_02_03_pow2_gb_120_5.csv\n",
      "Минимум предсказанных вероятностей = 0.00045628836046\n",
      "Максимум предсказанных вероятностей = 0.995179420703\n",
      "Done6 2016-05-04 03:42:17.861000\n",
      "--- Start test  2016-05-04 03:42:17.861000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (Before). parDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (After). rezDataFrame (17177, 187)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 187 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(104)\n",
      "memory usage: 24.6 MB\n",
      "None\n",
      "(17177, 187) <class 'pandas.core.frame.DataFrame'>\n",
      "(17177, 176) <class 'pandas.core.frame.DataFrame'>\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 2992286\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 3023152\n",
      "<type 'numpy.ndarray'>\n",
      "(17177L, 113L)\n",
      "<type 'numpy.ndarray'>\n",
      "(17177L, 10L)\n",
      "(17177L, 113L) (17177L, 10L)\n",
      "(17177L, 123L)\n",
      "added columns:  ['added0', 'added1', 'added2', 'added3', 'added4', 'added5', 'added6', 'added7', 'added8', 'added9', 'added10', 'added11', 'added12', 'added13', 'added14', 'added15', 'added16', 'added17', 'added18', 'added19', 'added20', 'added21', 'added22', 'added23', 'added24', 'added25', 'added26', 'added27', 'added28', 'added29', 'added30', 'added31', 'added32', 'added33', 'added34', 'added35', 'added36', 'added37', 'added38', 'added39', 'added40', 'added41', 'added42', 'added43', 'added44', 'added45', 'added46', 'added47', 'added48', 'added49', 'added50', 'added51', 'added52', 'added53', 'added54', 'added55', 'added56', 'added57', 'added58', 'added59', 'added60', 'added61', 'added62', 'added63', 'added64', 'added65', 'added66', 'added67', 'added68', 'added69', 'added70', 'added71', 'added72', 'added73', 'added74', 'added75', 'added76', 'added77', 'added78', 'added79', 'added80', 'added81', 'added82', 'added83', 'added84', 'added85', 'added86', 'added87', 'added88', 'added89', 'added90', 'added91', 'added92', 'added93', 'added94', 'added95', 'added96', 'added97', 'added98', 'added99', 'added100', 'added101', 'added102', 'added103', 'added104', 'added105', 'added106', 'added107', 'added108', 'added109', 'added110', 'added111', 'added112', 'added113', 'added114', 'added115', 'added116', 'added117', 'added118', 'added119', 'added120', 'added121', 'added122']\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "(17177, 299) <class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "=++++++++= X_test.shape  (17177, 300)\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/logreg_test_01_02_03_pow2_gb_120_5.csv\n",
      "Минимум предсказанных вероятностей = 0.0156655504299\n",
      "Максимум предсказанных вероятностей = 0.987560609034\n",
      "Done\n",
      "############## logistic regression gb_120_4\n",
      "added file TRAIN = C:/My/Dota2/03/ExportedRez_grad_boost/grad_boost_train_120_4.csv\n",
      "added file TEST = C:/My/Dota2/03/ExportedRez_grad_boost/grad_boost_test_120_4.csv\n",
      "---Start train 2016-05-04 03:43:09.054000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (Before). parDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (After). rezDataFrame (97230, 193)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 193 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(110)\n",
      "memory usage: 143.9 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 193)\n",
      "remove_result_columns (After): shape (97230, 187)\n",
      "(97230, 187) <class 'pandas.core.frame.DataFrame'>\n",
      "(97230, 176) <class 'pandas.core.frame.DataFrame'>\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 16938946\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 17112480\n",
      "<type 'numpy.ndarray'>\n",
      "(97230L, 113L)\n",
      "<type 'numpy.ndarray'>\n",
      "(97230L, 10L)\n",
      "(97230L, 113L) (97230L, 10L)\n",
      "(97230L, 123L)\n",
      "added columns:  ['added0', 'added1', 'added2', 'added3', 'added4', 'added5', 'added6', 'added7', 'added8', 'added9', 'added10', 'added11', 'added12', 'added13', 'added14', 'added15', 'added16', 'added17', 'added18', 'added19', 'added20', 'added21', 'added22', 'added23', 'added24', 'added25', 'added26', 'added27', 'added28', 'added29', 'added30', 'added31', 'added32', 'added33', 'added34', 'added35', 'added36', 'added37', 'added38', 'added39', 'added40', 'added41', 'added42', 'added43', 'added44', 'added45', 'added46', 'added47', 'added48', 'added49', 'added50', 'added51', 'added52', 'added53', 'added54', 'added55', 'added56', 'added57', 'added58', 'added59', 'added60', 'added61', 'added62', 'added63', 'added64', 'added65', 'added66', 'added67', 'added68', 'added69', 'added70', 'added71', 'added72', 'added73', 'added74', 'added75', 'added76', 'added77', 'added78', 'added79', 'added80', 'added81', 'added82', 'added83', 'added84', 'added85', 'added86', 'added87', 'added88', 'added89', 'added90', 'added91', 'added92', 'added93', 'added94', 'added95', 'added96', 'added97', 'added98', 'added99', 'added100', 'added101', 'added102', 'added103', 'added104', 'added105', 'added106', 'added107', 'added108', 'added109', 'added110', 'added111', 'added112', 'added113', 'added114', 'added115', 'added116', 'added117', 'added118', 'added119', 'added120', 'added121', 'added122']\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "(97230, 299) <class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "roc_auc_score= 0.746118851562\n",
      "Done0\n",
      "Done1\n",
      "Done2\n",
      "--- initial C calculation\n",
      "{'C': [1e-11, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 100000000]}\n",
      "best param =  {'C': 1000} best score =  0.779022694366\n",
      "mean: 0.53822, std: 0.00362, params: {'C': 1e-11}\n",
      "mean: 0.75510, std: 0.00379, params: {'C': 0.0001}\n",
      "mean: 0.77077, std: 0.00391, params: {'C': 0.001}\n",
      "mean: 0.77798, std: 0.00395, params: {'C': 0.01}\n",
      "mean: 0.77897, std: 0.00392, params: {'C': 0.1}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 1}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 10}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 100}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 1000}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 100000000}\n",
      "--- fine C tuning\n",
      "{'C': array([ 500,  600,  700,  800,  900, 1000, 1100, 1200, 1300, 1400, 1500])}\n",
      "best param =  {'C': 1300} best score =  0.779022709146\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 500}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 600}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 700}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 800}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 900}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 1000}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 1100}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 1200}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 1300}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 1400}\n",
      "mean: 0.77902, std: 0.00390, params: {'C': 1500}\n",
      "Average time for single regression fit: 0:03:05.840333\n",
      "Done3\n",
      "Done4\n",
      "Done5\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/logreg_train_01_02_03_pow2_gb_120_4.csv\n",
      "Минимум предсказанных вероятностей = 0.0123136105282\n",
      "Максимум предсказанных вероятностей = 0.98192887532\n",
      "Done6 2016-05-04 04:54:06.169000\n",
      "--- Start test  2016-05-04 04:54:06.169000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (Before). parDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (After). rezDataFrame (17177, 187)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 187 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(104)\n",
      "memory usage: 24.6 MB\n",
      "None\n",
      "(17177, 187) <class 'pandas.core.frame.DataFrame'>\n",
      "(17177, 176) <class 'pandas.core.frame.DataFrame'>\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 2992286\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 3023152\n",
      "<type 'numpy.ndarray'>\n",
      "(17177L, 113L)\n",
      "<type 'numpy.ndarray'>\n",
      "(17177L, 10L)\n",
      "(17177L, 113L) (17177L, 10L)\n",
      "(17177L, 123L)\n",
      "added columns:  ['added0', 'added1', 'added2', 'added3', 'added4', 'added5', 'added6', 'added7', 'added8', 'added9', 'added10', 'added11', 'added12', 'added13', 'added14', 'added15', 'added16', 'added17', 'added18', 'added19', 'added20', 'added21', 'added22', 'added23', 'added24', 'added25', 'added26', 'added27', 'added28', 'added29', 'added30', 'added31', 'added32', 'added33', 'added34', 'added35', 'added36', 'added37', 'added38', 'added39', 'added40', 'added41', 'added42', 'added43', 'added44', 'added45', 'added46', 'added47', 'added48', 'added49', 'added50', 'added51', 'added52', 'added53', 'added54', 'added55', 'added56', 'added57', 'added58', 'added59', 'added60', 'added61', 'added62', 'added63', 'added64', 'added65', 'added66', 'added67', 'added68', 'added69', 'added70', 'added71', 'added72', 'added73', 'added74', 'added75', 'added76', 'added77', 'added78', 'added79', 'added80', 'added81', 'added82', 'added83', 'added84', 'added85', 'added86', 'added87', 'added88', 'added89', 'added90', 'added91', 'added92', 'added93', 'added94', 'added95', 'added96', 'added97', 'added98', 'added99', 'added100', 'added101', 'added102', 'added103', 'added104', 'added105', 'added106', 'added107', 'added108', 'added109', 'added110', 'added111', 'added112', 'added113', 'added114', 'added115', 'added116', 'added117', 'added118', 'added119', 'added120', 'added121', 'added122']\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "(17177, 299) <class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "=++++++++= X_test.shape  (17177, 300)\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/logreg_test_01_02_03_pow2_gb_120_4.csv\n",
      "Минимум предсказанных вероятностей = 0.0239849169167\n",
      "Максимум предсказанных вероятностей = 0.982041882111\n",
      "Done\n",
      "############## logistic regression gb_10_2\n",
      "added file TRAIN = C:/My/Dota2/03/ExportedRez_grad_boost/grad_boost_train_10_2.csv\n",
      "added file TEST = C:/My/Dota2/03/ExportedRez_grad_boost/grad_boost_test_10_2.csv\n",
      "---Start train 2016-05-04 04:54:57.702000\n",
      "modify_dayaframe_01 (Before). parDataFrame (97230, 108)\n",
      "modify_dayaframe_01 (After). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (Before). parDataFrame (97230, 107)\n",
      "modify_dayaframe_03 (After). rezDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (Before). parDataFrame (97230, 121)\n",
      "modify_dayaframe_02 (After). rezDataFrame (97230, 193)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97230 entries, 0 to 114406\n",
      "Columns: 193 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(110)\n",
      "memory usage: 143.9 MB\n",
      "None\n",
      "remove_result_columns (Before): shape (97230, 193)\n",
      "remove_result_columns (After): shape (97230, 187)\n",
      "(97230, 187) <class 'pandas.core.frame.DataFrame'>\n",
      "(97230, 176) <class 'pandas.core.frame.DataFrame'>\n",
      "Before\n",
      "NaNs = 173534\n",
      "not NaNs = 16938946\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 17112480\n",
      "<type 'numpy.ndarray'>\n",
      "(97230L, 113L)\n",
      "<type 'numpy.ndarray'>\n",
      "(97230L, 10L)\n",
      "(97230L, 113L) (97230L, 10L)\n",
      "(97230L, 123L)\n",
      "added columns:  ['added0', 'added1', 'added2', 'added3', 'added4', 'added5', 'added6', 'added7', 'added8', 'added9', 'added10', 'added11', 'added12', 'added13', 'added14', 'added15', 'added16', 'added17', 'added18', 'added19', 'added20', 'added21', 'added22', 'added23', 'added24', 'added25', 'added26', 'added27', 'added28', 'added29', 'added30', 'added31', 'added32', 'added33', 'added34', 'added35', 'added36', 'added37', 'added38', 'added39', 'added40', 'added41', 'added42', 'added43', 'added44', 'added45', 'added46', 'added47', 'added48', 'added49', 'added50', 'added51', 'added52', 'added53', 'added54', 'added55', 'added56', 'added57', 'added58', 'added59', 'added60', 'added61', 'added62', 'added63', 'added64', 'added65', 'added66', 'added67', 'added68', 'added69', 'added70', 'added71', 'added72', 'added73', 'added74', 'added75', 'added76', 'added77', 'added78', 'added79', 'added80', 'added81', 'added82', 'added83', 'added84', 'added85', 'added86', 'added87', 'added88', 'added89', 'added90', 'added91', 'added92', 'added93', 'added94', 'added95', 'added96', 'added97', 'added98', 'added99', 'added100', 'added101', 'added102', 'added103', 'added104', 'added105', 'added106', 'added107', 'added108', 'added109', 'added110', 'added111', 'added112', 'added113', 'added114', 'added115', 'added116', 'added117', 'added118', 'added119', 'added120', 'added121', 'added122']\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "(97230, 299) <class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "roc_auc_score= 0.707206353776\n",
      "Done0\n",
      "Done1\n",
      "Done2\n",
      "--- initial C calculation\n",
      "{'C': [1e-11, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 100000000]}\n",
      "best param =  {'C': 0.01} best score =  0.753298289622\n",
      "mean: 0.53808, std: 0.00395, params: {'C': 1e-11}\n",
      "mean: 0.74819, std: 0.00370, params: {'C': 0.0001}\n",
      "mean: 0.75303, std: 0.00365, params: {'C': 0.001}\n",
      "mean: 0.75330, std: 0.00371, params: {'C': 0.01}\n",
      "mean: 0.75318, std: 0.00373, params: {'C': 0.1}\n",
      "mean: 0.75313, std: 0.00374, params: {'C': 1}\n",
      "mean: 0.75312, std: 0.00374, params: {'C': 10}\n",
      "mean: 0.75312, std: 0.00374, params: {'C': 100}\n",
      "mean: 0.75312, std: 0.00374, params: {'C': 1000}\n",
      "mean: 0.75312, std: 0.00374, params: {'C': 100000000}\n",
      "--- fine C tuning\n",
      "{'C': array([ 0.005,  0.006,  0.007,  0.008,  0.009,  0.01 ,  0.011,  0.012,\n",
      "        0.013,  0.014,  0.015])}\n",
      "best param =  {'C': 0.0090000000000000011} best score =  0.753300105233\n",
      "mean: 0.75329, std: 0.00369, params: {'C': 0.0050000000000000001}\n",
      "mean: 0.75330, std: 0.00370, params: {'C': 0.0060000000000000001}\n",
      "mean: 0.75330, std: 0.00370, params: {'C': 0.0070000000000000001}\n",
      "mean: 0.75330, std: 0.00371, params: {'C': 0.0080000000000000002}\n",
      "mean: 0.75330, std: 0.00371, params: {'C': 0.0090000000000000011}\n",
      "mean: 0.75330, std: 0.00371, params: {'C': 0.01}\n",
      "mean: 0.75330, std: 0.00371, params: {'C': 0.010999999999999999}\n",
      "mean: 0.75329, std: 0.00371, params: {'C': 0.012}\n",
      "mean: 0.75329, std: 0.00371, params: {'C': 0.013000000000000001}\n",
      "mean: 0.75329, std: 0.00372, params: {'C': 0.014}\n",
      "mean: 0.75328, std: 0.00372, params: {'C': 0.014999999999999999}\n",
      "Average time for single regression fit: 0:01:19.122238\n",
      "Done3\n",
      "Done4\n",
      "Done5\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/logreg_train_01_02_03_pow2_gb_10_2.csv\n",
      "Минимум предсказанных вероятностей = 0.00256856963414\n",
      "Максимум предсказанных вероятностей = 0.996878500539\n",
      "Done6 2016-05-04 05:27:54.877000\n",
      "--- Start test  2016-05-04 05:27:54.877000\n",
      "modify_dayaframe_01 (Before). parDataFrame (17177, 102)\n",
      "modify_dayaframe_01 (After). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (Before). parDataFrame (17177, 101)\n",
      "modify_dayaframe_03 (After). rezDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (Before). parDataFrame (17177, 115)\n",
      "modify_dayaframe_02 (After). rezDataFrame (17177, 187)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17177 entries, 6 to 114398\n",
      "Columns: 187 entries, start_time to d_itemspwr2\n",
      "dtypes: float64(83), int64(104)\n",
      "memory usage: 24.6 MB\n",
      "None\n",
      "(17177, 187) <class 'pandas.core.frame.DataFrame'>\n",
      "(17177, 176) <class 'pandas.core.frame.DataFrame'>\n",
      "Before\n",
      "NaNs = 30866\n",
      "not NaNs = 2992286\n",
      "after\n",
      "NaNs = 0\n",
      "not NaNs = 3023152\n",
      "<type 'numpy.ndarray'>\n",
      "(17177L, 113L)\n",
      "<type 'numpy.ndarray'>\n",
      "(17177L, 10L)\n",
      "(17177L, 113L) (17177L, 10L)\n",
      "(17177L, 123L)\n",
      "added columns:  ['added0', 'added1', 'added2', 'added3', 'added4', 'added5', 'added6', 'added7', 'added8', 'added9', 'added10', 'added11', 'added12', 'added13', 'added14', 'added15', 'added16', 'added17', 'added18', 'added19', 'added20', 'added21', 'added22', 'added23', 'added24', 'added25', 'added26', 'added27', 'added28', 'added29', 'added30', 'added31', 'added32', 'added33', 'added34', 'added35', 'added36', 'added37', 'added38', 'added39', 'added40', 'added41', 'added42', 'added43', 'added44', 'added45', 'added46', 'added47', 'added48', 'added49', 'added50', 'added51', 'added52', 'added53', 'added54', 'added55', 'added56', 'added57', 'added58', 'added59', 'added60', 'added61', 'added62', 'added63', 'added64', 'added65', 'added66', 'added67', 'added68', 'added69', 'added70', 'added71', 'added72', 'added73', 'added74', 'added75', 'added76', 'added77', 'added78', 'added79', 'added80', 'added81', 'added82', 'added83', 'added84', 'added85', 'added86', 'added87', 'added88', 'added89', 'added90', 'added91', 'added92', 'added93', 'added94', 'added95', 'added96', 'added97', 'added98', 'added99', 'added100', 'added101', 'added102', 'added103', 'added104', 'added105', 'added106', 'added107', 'added108', 'added109', 'added110', 'added111', 'added112', 'added113', 'added114', 'added115', 'added116', 'added117', 'added118', 'added119', 'added120', 'added121', 'added122']\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "(17177, 299) <class 'pandas.core.frame.DataFrame'>\n",
      "add_calculated_prob (Start)\n",
      "merge_dataframes_preserve_order (Start)\n",
      "merge_dataframes_preserve_order (End)\n",
      "add_calculated_prob (End)\n",
      "=++++++++= X_test.shape  (17177, 300)\n",
      "store_predicted_probability FILE= C:/My/Dota2/04/Rez01/logreg_test_01_02_03_pow2_gb_10_2.csv\n",
      "Минимум предсказанных вероятностей = 0.0141791343174\n",
      "Максимум предсказанных вероятностей = 0.995997002924\n",
      "Done\n",
      "Done _ all 2016-05-04 05:28:46.181000\n"
     ]
    }
   ],
   "source": [
    "print 'Start _ all', printtime()  \n",
    "'''\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 6\n",
    "0.722038617167\n",
    "\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 5\n",
    "0.723343393486\n",
    "\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 4\n",
    "0.722611854863\n",
    "\n",
    "\n",
    "gradient boosting: trainig. Estimators=  10 max_depth= 2\n",
    "0.703845172719\n",
    "'''\n",
    "\n",
    "log_reg_folder= './Dota2_03/ExportedRez_log_regeression/'\n",
    "\n",
    "additional_train_file = log_reg_folder + 'logreg_train_01_02_03_pow2.csv'\n",
    "additional_test_file = log_reg_folder + 'logreg_test_01_02_03_pow2.csv'\n",
    "\n",
    "gradient_boosting_train_test_plus_len_reg([120], 6, additional_train_file, additional_test_file)\n",
    "gradient_boosting_train_test_plus_len_reg([120], 5, additional_train_file, additional_test_file)\n",
    "gradient_boosting_train_test_plus_len_reg([120], 4, additional_train_file, additional_test_file)\n",
    "gradient_boosting_train_test_plus_len_reg([10], 2, additional_train_file, additional_test_file)\n",
    "\n",
    "\n",
    "print 'Done _ all', printtime()  \n",
    "\n",
    "\n",
    "# ########################################################################################################\n",
    "\n",
    "\n",
    "print 'Start _ all', printtime()  \n",
    "'''\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 6\n",
    "0.722038617167\n",
    "\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 5\n",
    "0.723343393486\n",
    "\n",
    "gradient boosting: trainig. Estimators=  120 max_depth= 4\n",
    "0.722611854863\n",
    "\n",
    "\n",
    "gradient boosting: trainig. Estimators=  10 max_depth= 2\n",
    "0.703845172719\n",
    "'''\n",
    "grad_boost_rez_folder='./Dota2_03/ExportedRez_grad_boost/'\n",
    "\n",
    "\n",
    "additional_train_file = grad_boost_rez_folder+'grad_boost_train_120_6.csv'\n",
    "additional_test_file = grad_boost_rez_folder+'grad_boost_test_120_6.csv'\n",
    "len_reg_train_test_plus_gradient_boosting('gb_120_6', additional_train_file, additional_test_file)\n",
    "\n",
    "\n",
    "additional_train_file = grad_boost_rez_folder+'grad_boost_train_120_5.csv'\n",
    "additional_test_file = grad_boost_rez_folder+'grad_boost_test_120_5.csv'\n",
    "len_reg_train_test_plus_gradient_boosting('gb_120_5', additional_train_file, additional_test_file)\n",
    "\n",
    "additional_train_file = grad_boost_rez_folder+'grad_boost_train_120_4.csv'\n",
    "additional_test_file = grad_boost_rez_folder+'grad_boost_test_120_4.csv'\n",
    "len_reg_train_test_plus_gradient_boosting('gb_120_4', additional_train_file, additional_test_file)\n",
    "\n",
    "\n",
    "additional_train_file = grad_boost_rez_folder+'grad_boost_train_10_2.csv'\n",
    "additional_test_file = grad_boost_rez_folder+'grad_boost_test_10_2.csv'\n",
    "len_reg_train_test_plus_gradient_boosting('gb_10_2', additional_train_file, additional_test_file)\n",
    "\n",
    "\n",
    "print 'Done _ all', printtime()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг - train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print 'Start' , printtime() \n",
    "x_train_bust, y_train_bust, x_train_data_frame_bust  = prepare_for_gradient_boosting(feature_train_file_config, False)\n",
    "print 'Done1'\n",
    "\n",
    "rez_scores = crossvalidate_estimators([50], x_train_bust, y_train_bust, max_depth=6)\n",
    "print 'Done2', printtime()   \n",
    "\n",
    "clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "clf.fit(x_train_bust, y_train_bust)\n",
    "print 'Done3', printtime()   \n",
    "\n",
    "rez_dataframe = store_predicted_probability(clf, x_train_bust, x_train_data_frame_bust, './Dota2_02/grad_boost_train_200_6.csv')\n",
    "print 'Done4', printtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_bust , y_bust, x_bust_dataframe = prepare_for_log_regr(feature_train_file_config, False)\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Считаем для 200 - max_depth=6\n",
    "rez_scores_6 = list()\n",
    "for n_estimators in [200]:    \n",
    "    rez_scores_6.append((n_estimators, calculate_scores_for_estimators(n_estimators, x_bust , y_bust, max_depth=6)))    \n",
    "rez_scores_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rez_scores = crossvalidate_estimators([300], x_bust, y_bust, max_depth=8)\n",
    "clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "clf.fit(x_bust, y_bust)\n",
    "rez_dataframe = store_predicted_probability(clf, x_bust, match_info, './Dota2_02/grad_bust_test_300_8.csv')\n",
    "print 'Done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rez_scores = crossvalidate_estimators([200], x_bust, y_bust, max_depth=6)\n",
    "clf= get_best_GradientBoostingClassifier(rez_scores)\n",
    "#clf.fit(x_bust, y_bust)\n",
    "#rez_dataframe = store_predicted_probability(clf, x_bust, x_bust_dataframe, '/Volumes/fast64/My/MachineLearning/week07/Dota2_04/grad_bust_train_200_6.csv')\n",
    "print 'Done', printtime()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rez_dataframe = store_predicted_probability(clf, x_bust, x_bust_dataframe, './Dota2_02/grad_bust_train_200_6.csv')\n",
    "print 'Done', printtime()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rez_dataframe = store_predicted_probability(clf, x_bust, x_bust_dataframe, './Dota2_02/grad_bust_train_200_6.csv')\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг - test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print 'Start', printtime()  \n",
    "x_test, X_test_data_frame = prepare_for_gradient_boosting(feature_test_file_config, True)\n",
    "\n",
    "rez_dataframe = store_predicted_probability(clf, x_test, X_test_data_frame, './Dota2_02/grad_bust_test_200_6.csv')\n",
    "print 'Done', printtime()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сливаем результаты в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# сливаем массив результирующих csv в один\n",
    "\n",
    "def join_all_results(files, rez_file_name):\n",
    "    src_list= list()\n",
    "    for fl in files:    \n",
    "        fl_content = pandas.read_csv(fl, index_col='match_id')\n",
    "        print fl_content.shape\n",
    "        src_list.append(fl_content)\n",
    "\n",
    "    lngth = len(src_list[0].index)\n",
    "    rez = [0] * lngth\n",
    "\n",
    "    src_num = len(src_list)\n",
    "\n",
    "    for sss in src_list:\n",
    "        rez = rez + sss['radiant_win']\n",
    "\n",
    "    rez =rez/src_num\n",
    "\n",
    "    test_check = {\n",
    "        'match_id': src_list[0].index, \n",
    "        'radiant_win': rez\n",
    "    }\n",
    "\n",
    "    test_check_dframe = pandas.DataFrame.from_dict(test_check)\n",
    "    test_check_dframe.set_index('match_id')\n",
    "\n",
    "    test_check_dframe.to_csv(rez_file_name, index=False, columns=['match_id', 'radiant_win'])\n",
    "    print 'done join_all_results'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "files = ['./Dota2_02/grad_bust_test_300_8.csv', \n",
    "        './Dota2_02/logreg_test.csv']\n",
    "rez_file_name = './Dota2_02/summary_test1.csv'\n",
    "\n",
    "join_all_results(files, rez_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Старое  --------\n",
    "# Старое  -------- \n",
    "# Старое  -------- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подход 1: градиентный бустинг \"в лоб\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Проверьте выборку на наличие пропусков с помощью функции count(), которая для каждого столбца показывает число заполненных значений. Много ли пропусков в данных? Запишите названия признаков, имеющих пропуски, и попробуйте для любых двух из них дать обоснование, почему их значения могут быть пропущены.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "match_info_count = match_info.count()\n",
    "missing_data_columns_info = match_info_count[match_info_count != match_info_count.max()]\n",
    "\n",
    "print \"Q: Много ли пропусков в данных?\"\n",
    "print \"A: \",len(missing_data_columns_info),\"признаков имеют пропуски в данных?\",\"\\n\"\n",
    "\n",
    "print  \"названия признаков, имеющих пропуски\"\n",
    "\n",
    "\n",
    "print missing_data_columns_info\n",
    "# !!!!!!!!!!! посчитать % пропуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Q: попробуйте для любых двух из них дать обоснование, почему их значения могут быть пропущены <br/>\n",
    "A:  <br/>\n",
    "поля <b>first_blood_time, first_blood_team, first_blood_player1</b><br/>\n",
    "не было первой крови в первые 5 минут\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# проверим, что нет признаков с пропусками\n",
    "X_count=X.count()\n",
    "ZZZ = X_count[X_count != X_count.max()]\n",
    "print \"Количество признаков с пропусками после заполнения =\", ZZZ.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Какой столбец содержит целевую переменную? Запишите его название.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Q: Какой столбец содержит целевую переменную? Запишите его название. <br/>\n",
    "A:  <br/>\n",
    "столбец <b>radiant_win</b><br/>\n",
    "1, если победила команда Radiant, 0 — иначе\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Забудем, что в выборке есть категориальные признаки, и попробуем обучить градиентный бустинг над деревьями на имеющейся матрице \"объекты-признаки\". Зафиксируйте генератор разбиений для кросс-валидации по 5 блокам (KFold), не забудьте перемешать при этом выборку (shuffle=True), поскольку данные в таблице отсортированы по времени, и без перемешивания можно столкнуться с нежелательными эффектами при оценивании качества. Оцените качество градиентного бустинга (GradientBoostingClassifier) с помощью данной кросс-валидации, попробуйте при этом разное количество деревьев (как минимум протестируйте следующие значения для количества деревьев: 10, 20, 30). Долго ли настраивались классификаторы? Достигнут ли оптимум на испытанных значениях параметра n_estimators, или же качество, скорее всего, продолжит расти при дальнейшем его увеличении?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "import datetime\n",
    "\n",
    "def calculate_scores_for_estimators(n_estimators, max_depth=3):\n",
    "    start_time = datetime.datetime.now()\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "        random_state=555, max_depth=max_depth,\n",
    "    )\n",
    "    k_fold = KFold(len(y), n_folds=5, random_state=555, shuffle=True)\n",
    "    local_scores = cross_val_score(clf, X, y, cv=k_fold, scoring='roc_auc')\n",
    "    print 'n_estimators =', n_estimators, 'max_depth=', max_depth, 'Time elapsed:', datetime.datetime.now() - start_time\n",
    "    return local_scores.mean(), local_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Считаем для 10,20,30, 100, 200 - max_depth=3\n",
    "rez_scores_3 = list()\n",
    "for n_estimators in [10,20,30,100, 200]:    \n",
    "    rez_scores_3.append((n_estimators, calculate_scores_for_estimators(n_estimators=n_estimators)))    \n",
    "rez_scores_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Считаем для 10,20,30 - max_depth=2\n",
    "rez_scores_2 = list()\n",
    "for n_estimators in [10,20,30]:    \n",
    "    rez_scores_2.append((n_estimators, calculate_scores_for_estimators(n_estimators=n_estimators, max_depth=2)))    \n",
    "rez_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Считаем для 10,20,30 - max_depth=6\n",
    "rez_scores_6 = list()\n",
    "for n_estimators in [10,20,30]:    \n",
    "    rez_scores_6.append((n_estimators, calculate_scores_for_estimators(n_estimators=n_estimators, max_depth=6)))    \n",
    "rez_scores_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подход 2: логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "1. Оцените качество логистической регрессии (sklearn.linear_model.LogisticRegression с L2-регуляризацией) с помощью кросс-валидации по той же схеме, которая использовалась для градиентного бустинга. Подберите при этом лучший параметр регуляризации (C). Какое наилучшее качество у вас получилось? Как оно соотносится с качеством градиентного бустинга? Чем вы можете объяснить эту разницу? Быстрее ли работает логистическая регрессия по сравнению с градиентным бустингом?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "calculate_logreg(X_std_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "4. Воспользуемся подходом \"мешок слов\" для кодирования информации о героях. Пусть всего в игре имеет N различных героев. Сформируем N признаков, при этом i-й будет равен нулю, если i-й герой не участвовал в матче; единице, если i-й герой играл за команду Radiant; минус единице, если i-й герой играл за команду Dire. Ниже вы можете найти код, который выполняет данной преобразование. Добавьте полученные признаки к числовым, которые вы использовали во втором пункте данного этапа.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_pick = np.zeros((X.shape[0], max(unique_heroes)))\n",
    "\n",
    "for i, match_id in enumerate(X.index):\n",
    "    for p in xrange(5):\n",
    "        X_pick[i, X.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "        X_pick[i, X.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print X_pick.shape, type(X_pick.shape)\n",
    "print X_categor_removed.shape, type(X_categor_removed)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_categor_removed_bag_words = hstack([X_categor_removed, X_pick]).toarray()\n",
    "print X_categor_removed_bag_words.shape, type(X_categor_removed_bag_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "5. Проведите кросс-валидацию для логистической регрессии на новой выборке с подбором лучшего параметра регуляризации. Какое получилось качество? Улучшилось ли оно? Чем вы можете это объяснить?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_categor_removed_bag_words_std_scal = StandardScaler().fit_transform(X_categor_removed_bag_words)\n",
    "calculate_logreg(X_categor_removed_bag_words_std_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# добавим мешок слов по комнате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "unique_lobbyes = np.unique(X['lobby_type'])\n",
    "print unique_lobbyes\n",
    "print 'unique_lobbyes.size=', unique_lobbyes.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''id,name\n",
    "-1,Invalid\n",
    "0,Public matchmaking\n",
    "1,Practice\n",
    "2,Tournament\n",
    "3,Tutorial\n",
    "4,Co-op with bots\n",
    "5,Team match\n",
    "6,Solo Queue\n",
    "7,Ranked\n",
    "8,Solo Mid 1vs1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_rooms = 10\n",
    "X_pick_room = np.zeros([X.shape[0], num_rooms])   # !!! two (())\n",
    "\n",
    "for i, match_id in enumerate(X.index):\n",
    "    for p in xrange(10):\n",
    "        X_pick_room[i, X.ix[match_id, 'lobby_type']+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print X_pick.shape\n",
    "print X_pick_room.shape\n",
    "X_pick_heroes_room = np.concatenate((X_pick , X_pick_room),  axis=1)\n",
    "print X_pick_heroes_room.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print X_pick_heroes_room.shape, type(X_pick_heroes_room.shape)\n",
    "print X_categor_removed.shape, type(X_categor_removed)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_categor_removed_bag_words_hero_room = hstack([X_categor_removed, X_pick_heroes_room]).toarray()\n",
    "print X_categor_removed_bag_words_hero_room.shape, type(X_categor_removed_bag_words_hero_room)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler().fit(X_categor_removed_bag_words_hero_room)\n",
    "ttt= ss.transform(X_categor_removed_bag_words_hero_room)\n",
    "print ttt.mean(), ttt.min(), ttt.max()\n",
    "X_categor_removed_bag_words_hero_room_std_scal = StandardScaler().fit_transform(X_categor_removed_bag_words_hero_room)\n",
    "print X_categor_removed_bag_words_hero_room_std_scal.mean(), X_categor_removed_bag_words_hero_room_std_scal.min(), X_categor_removed_bag_words_hero_room_std_scal.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_categor_removed_bag_words_hero_room_std_scal = StandardScaler().fit_transform(X_categor_removed_bag_words_hero_room)\n",
    "calculate_logreg(X_categor_removed_bag_words_hero_room_std_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "6. Постройте предсказания вероятностей победы команды Radiant для тестовой выборки с помощью лучшей из изученных моделей (лучшей с точки зрения AUC-ROC на кросс-валидации). Убедитесь, что предсказанные вероятности адекватные — находятся на отрезке [0, 1], не совпадают между собой (т.е. что модель не получилась константной).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features_test = pandas.read_csv(feature_test_file_config, index_col='match_id')\n",
    "print features_test.info()\n",
    "X_test = features_test.fillna(0)\n",
    "\n",
    "X_test_categor_removed= X_test.drop( heros+ ['lobby_type'], axis=1)\n",
    "\n",
    "test_unique_heroes = np.unique(X_test[heros])\n",
    "print test_unique_heroes\n",
    "print 'test_unique_heroes.size=', test_unique_heroes.size\n",
    "\n",
    "\n",
    "# X_test_categor_removed_std_scal = StandardScaler().fit_transform(X_test_categor_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_pick = np.zeros((X_test.shape[0], max(test_unique_heroes)))\n",
    "\n",
    "for i, match_id in enumerate(X_test.index):\n",
    "    for p in xrange(5):\n",
    "        X_pick[i, X_test.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "        X_pick[i, X_test.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_rooms = 10\n",
    "X_pick_room = np.zeros([X_test.shape[0], num_rooms])   # !!! two (())\n",
    "\n",
    "for i, match_id in enumerate(X_test.index):\n",
    "    for p in xrange(10):\n",
    "        X_pick_room[i, X_test.ix[match_id, 'lobby_type']+1] = 1\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_test_pick_heroes_room = np.concatenate((X_pick , X_pick_room),  axis=1)\n",
    "\n",
    "print X_test_pick_heroes_room.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print X_test_pick_heroes_room.shape\n",
    "print X_test_categor_removed.shape\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_test_categor_removed_bag_words = hstack([X_test_categor_removed, X_test_pick_heroes_room]).toarray()\n",
    "print X_test_categor_removed_bag_words.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_test_categor_removed_bag_words_std_scal = ss.transform(X_test_categor_removed_bag_words)\n",
    "print X_test_categor_removed_bag_words_std_scal.mean(), X_test_categor_removed_bag_words_std_scal.min(), X_test_categor_removed_bag_words_std_scal.max()\n",
    "\n",
    "ttt = StandardScaler().fit_transform(X_categor_removed_bag_words_hero_room)\n",
    "print ttt.mean(), ttt.min(), ttt.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', random_state=555, C=0.005)\n",
    "clf.fit(X_categor_removed_bag_words_hero_room_std_scal, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# smoke check\n",
    "rez=clf.predict_proba(X_categor_removed_bag_words_hero_room_std_scal)[:, 1]\n",
    "\n",
    "smoke_check = {\n",
    "    'match_id': X.index, \n",
    "    'radiant_win_prob': rez,\n",
    "    'radiant_win_actual': y[X.index]\n",
    "}\n",
    "\n",
    "smoke_check_dframe = pandas.DataFrame.from_dict(smoke_check)\n",
    "\n",
    "smoke_check_dframe.set_index('match_id')\n",
    "\n",
    "num_good_predictions=0\n",
    "num_bad_predictions=0\n",
    "for index, row in smoke_check_dframe.iterrows():\n",
    "    if (row.radiant_win_prob >= 0.5 and row.radiant_win_actual == 1 or \n",
    "       row.radiant_win_prob < 0.5 and row.radiant_win_actual == 0):\n",
    "        num_good_predictions=num_good_predictions+1\n",
    "    else:\n",
    "        num_bad_predictions=num_bad_predictions+1\n",
    "print 'num_good_predictions=', num_good_predictions, 'num_bad_predictions=', num_bad_predictions\n",
    "print 'ratio=', 100.0*num_good_predictions/(num_good_predictions+num_bad_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rez=clf.predict_proba(X_test_categor_removed_bag_words_std_scal)[:, 1]\n",
    "\n",
    "print \"Минимум предсказанных вероятностей =\",min(rez)\n",
    "print \"Максимум предсказанных вероятностей =\", max(rez)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_check = {\n",
    "    'match_id': X_test.index, \n",
    "    'radiant_win': rez\n",
    "}\n",
    "\n",
    "test_check_dframe = pandas.DataFrame.from_dict(test_check)\n",
    "test_check_dframe.set_index('match_id')\n",
    "\n",
    "test_check_dframe.to_csv('./Dota2_01/dota2-kaggle-01.csv', index=False, columns=['match_id', 'radiant_win'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
